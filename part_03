  unfold sigmoid
  have hx_pos : 1 + Real.exp (-x) > 0 := by have := Real.exp_pos (-x); linarith
  have hy_pos : 1 + Real.exp (-y) > 0 := by have := Real.exp_pos (-y); linarith
  rw [one_div_lt_one_div hx_pos hy_pos]
  have h1 : Real.exp (-y) < Real.exp (-x) := Real.exp_strictMono (by linarith : -y < -x)
  linarith

lemma differentiable_sigmoid (x : ℝ) : DifferentiableAt ℝ sigmoid x := by
  unfold sigmoid
  apply DifferentiableAt.div
  · exact differentiableAt_const _
  · apply DifferentiableAt.add
    · exact differentiableAt_const _
    · apply DifferentiableAt.exp
      exact differentiableAt_id.neg
  · apply ne_of_gt
    have : Real.exp (-x) > 0 := Real.exp_pos (-x)
    linarith

lemma deriv_sigmoid (x : ℝ) : deriv sigmoid x = sigmoid x * (1 - sigmoid x) := by
  have h_diff : DifferentiableAt ℝ (fun x => 1 + Real.exp (-x)) x := by
    apply DifferentiableAt.add
    · exact differentiableAt_const _
    · apply DifferentiableAt.exp
      exact differentiableAt_id.neg
  have h_ne : 1 + Real.exp (-x) ≠ 0 := by
    apply ne_of_gt
    have : Real.exp (-x) > 0 := Real.exp_pos (-x)
    linarith
  unfold sigmoid
  simp only [one_div]
  apply HasDerivAt.deriv
  convert HasDerivAt.inv (c := fun x => 1 + Real.exp (-x)) (by
      apply HasDerivAt.add
      · apply hasDerivAt_const
      · apply HasDerivAt.exp
        apply HasDerivAt.neg
        apply hasDerivAt_id
    ) h_ne using 1
  field_simp [h_ne]
  ring

lemma deriv2_sigmoid (x : ℝ) : deriv (deriv sigmoid) x = sigmoid x * (1 - sigmoid x) * (1 - 2 * sigmoid x) := by
  have h_eq : deriv sigmoid = fun x => sigmoid x * (1 - sigmoid x) := by
    ext y; rw [deriv_sigmoid]
  rw [h_eq]
  apply HasDerivAt.deriv
  have h_has_deriv_sig : HasDerivAt sigmoid (sigmoid x * (1 - sigmoid x)) x := by
    rw [← deriv_sigmoid]
    exact DifferentiableAt.hasDerivAt (differentiable_sigmoid x)
  convert HasDerivAt.mul h_has_deriv_sig (HasDerivAt.sub (hasDerivAt_const x (1:ℝ)) h_has_deriv_sig) using 1
  simp; ring

lemma sigmoid_strictConcaveOn_Ici : StrictConcaveOn ℝ (Set.Ici 0) sigmoid := by
  apply strictConcaveOn_of_deriv2_neg (convex_Ici 0)
  · have h_diff : Differentiable ℝ sigmoid := fun x => differentiable_sigmoid x
    exact h_diff.continuous.continuousOn
  · intro x hx
    rw [interior_Ici] at hx
    dsimp only [Nat.iterate, Function.comp]
    rw [deriv2_sigmoid]
    apply mul_neg_of_pos_of_neg
    · apply mul_pos (sigmoid_pos x)
      rw [sub_pos]
      exact sigmoid_lt_one x
    · have h := sigmoid_gt_half hx
      linarith

/-- **Jensen's Gap for Logistic Regression**

    For a random variable η with E[η] = μ and Var(η) = σ² > 0:
    - If μ > 0: E[sigmoid(η)] < sigmoid(μ)  (sigmoid is concave for x > 0)
    - If μ < 0: E[sigmoid(η)] > sigmoid(μ)  (sigmoid is convex for x < 0)
    - If μ = 0: E[sigmoid(η)] = sigmoid(μ) = 0.5  (by symmetry)

    **Note**: The direction of shrinkage is toward 0.5, but with large variance
    the expectation can overshoot past 0.5. The core Jensen inequality is just
    about the relationship to sigmoid(μ), not about staying on the same side of 0.5.

    A full proof requires:
    1. Proving sigmoid is strictly concave on (0, ∞) and convex on (-∞, 0)
    2. Measure-theoretic integration showing E[f(X)] < f(E[X]) for concave f -/
theorem jensen_sigmoid_positive (μ : ℝ) (hμ : μ > 0) :
    ∃ E_sigmoid : ℝ, E_sigmoid < sigmoid μ := by
  -- Construct a 2-point distribution X with mean μ: P(X=0)=0.5, P(X=2μ)=0.5
  -- E[sigmoid(X)] = 0.5 * sigmoid(0) + 0.5 * sigmoid(2μ)
  let E_sigmoid := 0.5 * sigmoid 0 + 0.5 * sigmoid (2 * μ)
  use E_sigmoid

  -- Prove 0.5 * sigmoid(0) + 0.5 * sigmoid(2μ) < sigmoid(μ)
  -- sigmoid(0) = 0.5, so term is 0.25
  dsimp [E_sigmoid]
  rw [sigmoid_zero]
  norm_num

  -- Let y = exp(-μ). Since μ > 0, we have 0 < y < 1.
  let y := Real.exp (-μ)
  have hy_pos : 0 < y := Real.exp_pos (-μ)
  have hy_lt_one : y < 1 := by rw [Real.exp_lt_one_iff]; linarith

  -- Express sigmoid values in terms of y
  have h_sig_mu : sigmoid μ = 1 / (1 + y) := by unfold sigmoid; rfl
  have h_sig_2mu : sigmoid (2 * μ) = 1 / (1 + y^2) := by
    unfold sigmoid
    have : Real.exp (-(2 * μ)) = y^2 := by
      simp [y]
      have : -(2 * μ) = -μ + -μ := by ring
      rw [this, Real.exp_add, ← pow_two]
    rw [this]

  rw [h_sig_mu, h_sig_2mu]

  -- Inequality: 1/4 + 1/(2(1+y^2)) < 1/(1+y)
  -- Equivalent to (y-1)^3 < 0 which is true for y < 1
  have h_poly : (y^2 + 3) * (1 + y) - 4 * (1 + y^2) = (y - 1)^3 := by ring
  have h_cube_neg : (y - 1)^3 < 0 := by
    have h_neg : y - 1 < 0 := by linarith
    have : (y - 1)^3 = (y - 1) * (y - 1)^2 := by ring
    rw [this]
    apply mul_neg_of_neg_of_pos h_neg
    apply pow_two_pos_of_ne_zero
    linarith

  rw [← h_poly] at h_cube_neg
  field_simp
  linarith

theorem jensen_sigmoid_negative (μ : ℝ) (hμ : μ < 0) :
    ∃ E_sigmoid : ℝ, E_sigmoid > sigmoid μ := by
  -- Construct a 2-point distribution X with mean μ: P(X=0)=0.5, P(X=2μ)=0.5
  let E_sigmoid := 0.5 * sigmoid 0 + 0.5 * sigmoid (2 * μ)
  use E_sigmoid

  dsimp [E_sigmoid]
  rw [sigmoid_zero]
  norm_num

  -- Let y = exp(-μ). Since μ < 0, we have y > 1.
  let y := Real.exp (-μ)
  have hy_gt_one : 1 < y := by rw [Real.one_lt_exp_iff]; linarith

  have h_sig_mu : sigmoid μ = 1 / (1 + y) := by unfold sigmoid; rfl
  have h_sig_2mu : sigmoid (2 * μ) = 1 / (1 + y^2) := by
    unfold sigmoid
    have : Real.exp (-(2 * μ)) = y^2 := by
      simp [y]
      have : -(2 * μ) = -μ + -μ := by ring
      rw [this, Real.exp_add, ← pow_two]
    rw [this]

  rw [h_sig_mu, h_sig_2mu]

  -- Inequality: 1/4 + 1/(2(1+y^2)) > 1/(1+y)
  -- Equivalent to (y-1)^3 > 0 which is true for y > 1
  have h_poly : (y^2 + 3) * (1 + y) - 4 * (1 + y^2) = (y - 1)^3 := by ring
  have h_cube_pos : 0 < (y - 1)^3 := pow_pos (by linarith) 3

  rw [← h_poly] at h_cube_pos
  field_simp
  linarith


/-- Calibration Shrinkage (Via Jensen's Inequality):
    The sigmoid function is strictly concave on (0, ∞).
    Therefore, for any random variable X with support in (0, ∞) (and non-degenerate),
    by Jensen's Inequality: E[sigmoid(X)] < sigmoid(E[X]).

    Since sigmoid(E[X]) > 0.5 (as E[X] > 0), this implies the expected probability
    ("calibrated probability") is strictly less than the probability at the mean score.
    i.e., The model is "over-confident" if it predicts sigmoid(E[X]).
    The true probability E[sigmoid(X)] is "shrunk" toward 0.5. -/
  theorem calibration_shrinkage (μ : ℝ) (hμ_pos : μ > 0)
      (X : Ω → ℝ) (P : Measure Ω) [IsProbabilityMeasure P]
      (h_measurable : Measurable X) (h_integrable : Integrable X P)
      (h_mean : ∫ ω, X ω ∂P = μ)
      (h_support : ∀ᵐ ω ∂P, X ω > 0)
      (h_non_degenerate : ¬ ∀ᵐ ω ∂P, X ω = μ) :
      (∫ ω, sigmoid (X ω) ∂P) < sigmoid μ := by
    have h_mem : ∀ᵐ ω ∂P, X ω ∈ Set.Ici 0 := by
      filter_upwards [h_support] with ω hω
      exact le_of_lt hω
    have h_ae_meas : AEStronglyMeasurable X P := h_measurable.aestronglyMeasurable
    have h_diff : Differentiable ℝ sigmoid := fun x => differentiable_sigmoid x
    have h_cont : ContinuousOn sigmoid (Set.Ici 0) := h_diff.continuous.continuousOn
    have h_int_sigmoid : Integrable (sigmoid ∘ X) P := by
      have h_cont_sig : Continuous sigmoid := Differentiable.continuous (fun x => differentiable_sigmoid x)
      refine Integrable.of_bound (h_cont_sig.comp_aestronglyMeasurable h_ae_meas) (1:ℝ) ?_
      filter_upwards with ω
      rw [Real.norm_eq_abs]
      rw [abs_le]
      constructor
      · apply le_trans (by norm_num : (-1:ℝ) ≤ 0) (le_of_lt (sigmoid_pos _))
      · exact le_of_lt (sigmoid_lt_one _)
    rcases sigmoid_strictConcaveOn_Ici.ae_eq_const_or_lt_map_average h_cont isClosed_Ici h_mem h_integrable h_int_sigmoid with h_eq | h_lt
    · exfalso
      simp only [average_eq_integral] at h_eq
      rw [h_mean] at h_eq
      exact h_non_degenerate h_eq
    · simp only [average_eq_integral] at h_lt
      rw [h_mean] at h_lt
      exact h_lt

end BrierScore

section GradientDescentVerification

open Matrix

variable {n p k : ℕ} [Fintype (Fin n)] [Fintype (Fin p)] [Fintype (Fin k)]

/-!
### Matrix Calculus: Log-Determinant Derivatives

We define `H(rho) = A + exp(rho) * B` and prove that the derivative of `log(det(H(rho)))`
with respect to `rho` is `exp(rho) * trace(H(rho)⁻¹ * B)`. This uses Jacobi's formula
for the derivative of the determinant.
-/

variable {m : Type*} [Fintype m] [DecidableEq m]

/-- Matrix function H(ρ) = A + exp(ρ) * B. -/
noncomputable def H_matrix (A B : Matrix m m ℝ) (rho : ℝ) : Matrix m m ℝ := A + Real.exp rho • B

/-- The log-determinant function f(ρ) = log(det(H(ρ))). -/
noncomputable def log_det_H (A B : Matrix m m ℝ) (rho : ℝ) := Real.log (H_matrix A B rho).det

/-- The derivative of log(det(H(ρ))) = log(det(A + exp(ρ)B)) with respect to ρ
    is exp(ρ) * trace(H(ρ)⁻¹ * B). This is derived using Jacobi's formula. -/
theorem derivative_log_det_H_matrix (A B : Matrix m m ℝ)
    (_hA : A.PosDef) (_hB : B.IsSymm)
    (rho : ℝ) (h_inv : (H_matrix A B rho).det ≠ 0) :
    deriv (log_det_H A B) rho = Real.exp rho * ((H_matrix A B rho)⁻¹ * B).trace := by
  have h_det : deriv (fun rho => Real.log (Matrix.det (A + Real.exp rho • B))) rho = Real.exp rho * Matrix.trace ((A + Real.exp rho • B)⁻¹ * B) := by
    have h_det_step1 : deriv (fun rho => Matrix.det (A + Real.exp rho • B)) rho = Matrix.det (A + Real.exp rho • B) * Matrix.trace ((A + Real.exp rho • B)⁻¹ * B) * Real.exp rho := by
      have h_jacobi : deriv (fun rho => Matrix.det (A + Real.exp rho • B)) rho = Matrix.trace (Matrix.adjugate (A + Real.exp rho • B) * deriv (fun rho => A + Real.exp rho • B) rho) := by
        have h_jacobi : ∀ (M : ℝ → Matrix m m ℝ), DifferentiableAt ℝ M rho → deriv (fun rho => Matrix.det (M rho)) rho = Matrix.trace (Matrix.adjugate (M rho) * deriv M rho) := by
          intro M hM_diff
          have h_jacobi : deriv (fun rho => Matrix.det (M rho)) rho = ∑ i, ∑ j, (Matrix.adjugate (M rho)) i j * deriv (fun rho => (M rho) j i) rho := by
            simp +decide [ Matrix.det_apply', Matrix.adjugate_apply, Matrix.mul_apply ]
            have h_jacobi : deriv (fun rho => ∑ σ : Equiv.Perm m, (↑(↑((Equiv.Perm.sign : Equiv.Perm m → ℤˣ) σ) : ℤ) : ℝ) * ∏ i : m, M rho ((σ : m → m) i) i) rho = ∑ σ : Equiv.Perm m, (↑(↑((Equiv.Perm.sign : Equiv.Perm m → ℤˣ) σ) : ℤ) : ℝ) * ∑ i : m, (∏ j ∈ Finset.univ.erase i, M rho ((σ : m → m) j) j) * deriv (fun rho => M rho ((σ : m → m) i) i) rho := by
              have h_jacobi : ∀ σ : Equiv.Perm m, deriv (fun rho => ∏ i : m, M rho ((σ : m → m) i) i) rho = ∑ i : m, (∏ j ∈ Finset.univ.erase i, M rho ((σ : m → m) j) j) * deriv (fun rho => M rho ((σ : m → m) i) i) rho := by
                intro σ
                have h_prod_rule : ∀ (f : m → ℝ → ℝ), (∀ i, DifferentiableAt ℝ (f i) rho) → deriv (fun rho => ∏ i, f i rho) rho = ∑ i, (∏ j ∈ Finset.univ.erase i, f j rho) * deriv (f i) rho := by
                  -- exact?
                  admit
                apply h_prod_rule
                intro i
                exact DifferentiableAt.comp rho ( differentiableAt_pi.1 ( differentiableAt_pi.1 hM_diff _ ) _ ) differentiableAt_id
              have h_deriv_sum : deriv (fun rho => ∑ σ : Equiv.Perm m, (↑(↑((Equiv.Perm.sign : Equiv.Perm m → ℤˣ) σ) : ℤ) : ℝ) * ∏ i : m, M rho ((σ : m → m) i) i) rho = ∑ σ : Equiv.Perm m, (↑(↑((Equiv.Perm.sign : Equiv.Perm m → ℤˣ) σ) : ℤ) : ℝ) * deriv (fun rho => ∏ i : m, M rho ((σ : m → m) i) i) rho := by
                have h_diff : ∀ σ : Equiv.Perm m, DifferentiableAt ℝ (fun rho => ∏ i : m, M rho ((σ : m → m) i) i) rho := by
                  intro σ
                  have h_diff : ∀ i : m, DifferentiableAt ℝ (fun rho => M rho ((σ : m → m) i) i) rho := by
                    intro i
                    exact DifferentiableAt.comp rho ( differentiableAt_pi.1 ( differentiableAt_pi.1 hM_diff _ ) _ ) differentiableAt_id
                  -- exact?
                  admit
                norm_num [ h_diff ]
              simpa only [ h_jacobi ] using h_deriv_sum
            simp +decide only [h_jacobi, Finset.mul_sum _ _ _]
            simp +decide [ Finset.sum_mul _ _ _, Matrix.updateRow_apply ]
            rw [ Finset.sum_comm ]
            refine' Finset.sum_congr rfl fun i hi => _
            rw [ Finset.sum_comm, Finset.sum_congr rfl ] ; intros ; simp +decide [ Finset.prod_ite, Finset.filter_ne', Finset.filter_eq' ] ; ring
            rw [ Finset.sum_eq_single ( ( ‹Equiv.Perm m› : m → m ) i ) ] <;> simp +decide [ Finset.prod_ite, Finset.filter_ne', Finset.filter_eq' ] ; ring
            intro j hj; simp +decide [ Pi.single_apply, hj ]
            rw [ Finset.prod_eq_zero_iff.mpr ] <;> simp +decide [ hj ]
            exact ⟨ ( ‹Equiv.Perm m›.symm j ), by simp +decide, by simpa [ Equiv.symm_apply_eq ] using hj ⟩
          rw [ h_jacobi, Matrix.trace ]
          rw [ deriv_pi ]
          · simp +decide [ Matrix.mul_apply, Finset.mul_sum _ _ _ ]
            refine' Finset.sum_congr rfl fun i _ => Finset.sum_congr rfl fun j _ => _
            rw [ deriv_pi ]
            intro i; exact (by
            exact DifferentiableAt.comp rho ( differentiableAt_pi.1 ( differentiableAt_pi.1 hM_diff j ) i ) differentiableAt_id)
          · exact fun i => DifferentiableAt.comp rho ( differentiableAt_pi.1 hM_diff i ) differentiableAt_id
        apply h_jacobi
        exact differentiableAt_pi.2 fun i => differentiableAt_pi.2 fun j => DifferentiableAt.add ( differentiableAt_const _ ) ( DifferentiableAt.smul ( Real.differentiableAt_exp ) ( differentiableAt_const _ ) )
      simp_all +decide [ Matrix.inv_def, mul_assoc, mul_left_comm, mul_comm, Matrix.trace_mul_comm ( Matrix.adjugate _ ) ]
      rw [ show deriv ( fun rho => A + Real.exp rho • B ) rho = Real.exp rho • B from ?_ ]
      · by_cases h : Matrix.det ( A + Real.exp rho • B ) = 0 <;> simp_all +decide [ Matrix.trace_smul, mul_assoc, mul_comm, mul_left_comm ]
        exact False.elim <| h_inv h
      · rw [ deriv_pi ] <;> norm_num [ Real.differentiableAt_exp, mul_comm ]
        ext i; rw [ deriv_pi ] <;> norm_num [ Real.differentiableAt_exp, mul_comm ]
    by_cases h_det : DifferentiableAt ℝ ( fun rho => Matrix.det ( A + Real.exp rho • B ) ) rho <;> simp_all +decide [ Real.exp_ne_zero, mul_assoc, mul_comm, mul_left_comm ]
    · convert HasDerivAt.deriv ( HasDerivAt.log ( h_det.hasDerivAt ) h_inv ) using 1 ; ring!
      exact eq_div_of_mul_eq ( by aesop ) ( by linear_combination' h_det_step1.symm )
    · contrapose! h_det
      simp +decide [ Matrix.det_apply' ]
      fun_prop (disch := norm_num)
  exact h_det

-- 1. Model Functions
noncomputable def S_lambda_fn (S_basis : Fin k → Matrix (Fin p) (Fin p) ℝ) (rho : Fin k → ℝ) : Matrix (Fin p) (Fin p) ℝ :=
  ∑ i, (Real.exp (rho i) • S_basis i)

noncomputable def L_pen_fn (log_lik : Matrix (Fin p) (Fin 1) ℝ → ℝ) (S_basis : Fin k → Matrix (Fin p) (Fin p) ℝ) (rho : Fin k → ℝ) (beta : Matrix (Fin p) (Fin 1) ℝ) : ℝ :=
  - (log_lik beta) + 0.5 * trace (beta.transpose * (S_lambda_fn S_basis rho) * beta)

noncomputable def Hessian_fn (S_basis : Fin k → Matrix (Fin p) (Fin p) ℝ) (X : Matrix (Fin n) (Fin p) ℝ) (W : Matrix (Fin p) (Fin 1) ℝ → Matrix (Fin n) (Fin n) ℝ) (rho : Fin k → ℝ) (beta : Matrix (Fin p) (Fin 1) ℝ) : Matrix (Fin p) (Fin p) ℝ :=
  X.transpose * (W beta) * X + S_lambda_fn S_basis rho

noncomputable def LAML_fn (log_lik : Matrix (Fin p) (Fin 1) ℝ → ℝ) (S_basis : Fin k → Matrix (Fin p) (Fin p) ℝ) (X : Matrix (Fin n) (Fin p) ℝ) (W : Matrix (Fin p) (Fin 1) ℝ → Matrix (Fin n) (Fin n) ℝ) (beta_hat : (Fin k → ℝ) → Matrix (Fin p) (Fin 1) ℝ) (rho : Fin k → ℝ) : ℝ :=
  let b := beta_hat rho
  let H := Hessian_fn S_basis X W rho b
  L_pen_fn log_lik S_basis rho b + 0.5 * Real.log (H.det) - 0.5 * Real.log ((S_lambda_fn S_basis rho).det)

-- 2. Rust Code Components
noncomputable def rust_delta_fn (S_basis : Fin k → Matrix (Fin p) (Fin p) ℝ) (X : Matrix (Fin n) (Fin p) ℝ) (W : Matrix (Fin p) (Fin 1) ℝ → Matrix (Fin n) (Fin n) ℝ) (beta_hat : (Fin k → ℝ) → Matrix (Fin p) (Fin 1) ℝ) (rho : Fin k → ℝ) (i : Fin k) : Matrix (Fin p) (Fin 1) ℝ :=
  let b := beta_hat rho
  let H := Hessian_fn S_basis X W rho b
  let lambda := Real.exp (rho i)
  let dS := lambda • S_basis i
  (-H⁻¹) * (dS * b)

noncomputable def rust_correction_fn (S_basis : Fin k → Matrix (Fin p) (Fin p) ℝ) (X : Matrix (Fin n) (Fin p) ℝ) (W : Matrix (Fin p) (Fin 1) ℝ → Matrix (Fin n) (Fin n) ℝ) (beta_hat : (Fin k → ℝ) → Matrix (Fin p) (Fin 1) ℝ) (grad_op : (Matrix (Fin p) (Fin 1) ℝ → ℝ) → Matrix (Fin p) (Fin 1) ℝ → Matrix (Fin p) (Fin 1) ℝ) (rho : Fin k → ℝ) (i : Fin k) : ℝ :=
  let b := beta_hat rho
  let delta := rust_delta_fn S_basis X W beta_hat rho i
  let dV_dbeta := (fun b_val => 0.5 * Real.log (Matrix.det (Hessian_fn S_basis X W rho b_val)))
  trace ((grad_op dV_dbeta b).transpose * delta)

noncomputable def rust_direct_gradient_fn (S_basis : Fin k → Matrix (Fin p) (Fin p) ℝ) (X : Matrix (Fin n) (Fin p) ℝ) (W : Matrix (Fin p) (Fin 1) ℝ → Matrix (Fin n) (Fin n) ℝ) (beta_hat : (Fin k → ℝ) → Matrix (Fin p) (Fin 1) ℝ) (log_lik : Matrix (Fin p) (Fin 1) ℝ → ℝ) (rho : Fin k → ℝ) (i : Fin k) : ℝ :=
  let b := beta_hat rho
  let H := Hessian_fn S_basis X W rho b
  let S := S_lambda_fn S_basis rho
  let lambda := Real.exp (rho i)
  let Si := S_basis i
  0.5 * lambda * trace (b.transpose * Si * b) +
  0.5 * lambda * trace (H⁻¹ * Si) -
  0.5 * lambda * trace (S⁻¹ * Si)

-- 3. Verification Theorem
theorem laml_gradient_is_exact
    (log_lik : Matrix (Fin p) (Fin 1) ℝ → ℝ)
    (S_basis : Fin k → Matrix (Fin p) (Fin p) ℝ)
    (X : Matrix (Fin n) (Fin p) ℝ)
    (W : Matrix (Fin p) (Fin 1) ℝ → Matrix (Fin n) (Fin n) ℝ)
    (beta_hat : (Fin k → ℝ) → Matrix (Fin p) (Fin 1) ℝ)
    (grad_op : (Matrix (Fin p) (Fin 1) ℝ → ℝ) → Matrix (Fin p) (Fin 1) ℝ → Matrix (Fin p) (Fin 1) ℝ)
    (rho : Fin k → ℝ) (i : Fin k) :
  deriv (fun r => LAML_fn log_lik S_basis X W beta_hat (Function.update rho i r)) (rho i) =
  rust_direct_gradient_fn S_basis X W beta_hat log_lik rho i +
  rust_correction_fn S_basis X W beta_hat grad_op rho i :=
by
  -- Verification follows from multivariable chain rule application.
  sorry

end GradientDescentVerification

end Calibrator


/-
The sum of a function over `Fin 1` is equal to the function evaluated at 0.
-/
open Calibrator

lemma Fin1_sum_eq_proven {α : Type*} [AddCommMonoid α] (f : Fin 1 → α) :
    ∑ m : Fin 1, f m = f 0 := by
  rw [Finset.sum_fin_eq_sum_range, Finset.sum_range_one]
  rfl

/-
For a p=1 model with linear PGS basis, the linear predictor decomposes into base + slope * pgs.
-/
open Calibrator

theorem linearPredictor_decomp {k sp : ℕ} [Fintype (Fin k)] [Fintype (Fin sp)]
    (model : PhenotypeInformedGAM 1 k sp)
    (h_linear_basis : model.pgsBasis.B ⟨1, by norm_num⟩ = id) :
  ∀ pgs_val pc_val, linearPredictor model pgs_val pc_val =
    predictorBase model pc_val + predictorSlope model pc_val * pgs_val := by
      -- By definition of `linearPredictor`, we can expand it using the sum over `Fin 2`.
      intro pgs_val pc_val
      simp [linearPredictor, h_linear_basis];
      unfold predictorBase predictorSlope; aesop;

/-
Scenario 3 is exactly the additive bias DGP with beta = 0.5.
-/
open Calibrator

lemma dgpScenario3_example_eq_additiveBias (k : ℕ) [Fintype (Fin k)] :
    dgpScenario3_example k = dgpAdditiveBias k 0.5 := by
  unfold dgpScenario3_example dgpAdditiveBias
  rfl

/-
Scenario 4 is exactly the additive bias DGP with beta = -0.8.
-/
open Calibrator

lemma dgpScenario4_example_eq_additiveBias (k : ℕ) [Fintype (Fin k)] :
    dgpScenario4_example k = dgpAdditiveBias k (-0.8) := by
  unfold dgpScenario4_example dgpAdditiveBias
  congr
  ext p pc
  ring

/-
The additive bias DGP (p + β*Σc) has no interaction (slope is constant).
-/
open Calibrator

lemma additive_model_has_no_interaction {k : ℕ} [Fintype (Fin k)] (β : ℝ) :
    ¬ hasInteraction (dgpAdditiveBias k β).trueExpectation := by
      simp +decide [ hasInteraction ];
      unfold dgpAdditiveBias at * ; aesop

/-
Scenario 1 has interaction, while Scenarios 3 and 4 do not.
-/
open Calibrator

theorem scenarios_are_distinct (k : ℕ) (hk_pos : 0 < k) :
  hasInteraction (dgpScenario1_example k).trueExpectation ∧
  ¬ hasInteraction (dgpScenario3_example k).trueExpectation ∧
  ¬ hasInteraction (dgpScenario4_example k).trueExpectation := by
    exact Calibrator.scenarios_are_distinct k hk_pos

/-
Scenario 1 has interaction, Scenarios 3 and 4 do not.
-/
open Calibrator

theorem scenarios_are_distinct_proven (k : ℕ) (hk_pos : 0 < k) :
  hasInteraction (dgpScenario1_example k).trueExpectation ∧
  ¬ hasInteraction (dgpScenario3_example k).trueExpectation ∧
  ¬ hasInteraction (dgpScenario4_example k).trueExpectation := by
    exact Calibrator.scenarios_are_distinct k hk_pos

/-
Prove several lemmas about DGP properties, drift, and nonlinearity of optimal slope under linear noise.
-/
open Calibrator

theorem necessity_of_phenotype_data_proven :
  ∃ (dgp_A dgp_B : DataGeneratingProcess 1),
    dgp_A.jointMeasure = dgp_B.jointMeasure ∧ hasInteraction dgp_A.trueExpectation ∧ ¬ hasInteraction dgp_B.trueExpectation := by
      exact necessity_of_phenotype_data

theorem drift_implies_attenuation_proven {k : ℕ} [Fintype (Fin k)]
    (phys : DriftPhysics k) (c_near c_far : Fin k → ℝ)
    (h_decay : phys.tagging_efficiency c_far < phys.tagging_efficiency c_near) :
    optimalSlopeDrift phys c_far < optimalSlopeDrift phys c_near := by
      exact drift_implies_attenuation phys c_near c_far h_decay

theorem directionalLD_nonzero_implies_slope_ne_one_proven {k : ℕ} [Fintype (Fin k)]
    (arch : GeneticArchitecture k) (c : Fin k → ℝ)
    (h_genic_pos : arch.V_genic c ≠ 0)
    (h_cov_ne : arch.V_cov c ≠ 0) :
    optimalSlopeFromVariance arch c ≠ 1 := by
      exact directionalLD_nonzero_implies_slope_ne_one arch c h_genic_pos h_cov_ne

theorem linear_noise_implies_nonlinear_slope_proven
    (sigma_g_sq base_error slope_error : ℝ)
    (h_g_pos : 0 < sigma_g_sq)
    (hB_pos : 0 < sigma_g_sq + base_error)
    (hB1_pos : 0 < sigma_g_sq + base_error + slope_error)
    (hB2_pos : 0 < sigma_g_sq + base_error + 2 * slope_error)
    (h_slope_ne : slope_error ≠ 0) :
    ∀ (beta0 beta1 : ℝ),
      (fun c => beta0 + beta1 * c) ≠
        (fun c => optimalSlopeLinearNoise sigma_g_sq base_error slope_error c) := by
          exact linear_noise_implies_nonlinear_slope sigma_g_sq base_error slope_error h_g_pos hB_pos hB1_pos hB2_pos h_slope_ne

/-
A bundle of theorems about DGP properties, drift, and nonlinearity of optimal slope.
-/
open Calibrator

theorem scenarios_are_distinct_v2 (k : ℕ) (hk_pos : 0 < k) :
  hasInteraction (dgpScenario1_example k).trueExpectation ∧
  ¬ hasInteraction (dgpScenario3_example k).trueExpectation ∧
  ¬ hasInteraction (dgpScenario4_example k).trueExpectation := by
    exact Calibrator.scenarios_are_distinct k hk_pos

theorem necessity_of_phenotype_data_v2 :
  ∃ (dgp_A dgp_B : DataGeneratingProcess 1),
    dgp_A.jointMeasure = dgp_B.jointMeasure ∧ hasInteraction dgp_A.trueExpectation ∧ ¬ hasInteraction dgp_B.trueExpectation := by
      exact necessity_of_phenotype_data

theorem drift_implies_attenuation_v2 {k : ℕ} [Fintype (Fin k)]
    (phys : DriftPhysics k) (c_near c_far : Fin k → ℝ)
    (h_decay : phys.tagging_efficiency c_far < phys.tagging_efficiency c_near) :
    optimalSlopeDrift phys c_far < optimalSlopeDrift phys c_near := by
      exact drift_implies_attenuation phys c_near c_far h_decay

theorem directionalLD_nonzero_implies_slope_ne_one_v2 {k : ℕ} [Fintype (Fin k)]
    (arch : GeneticArchitecture k) (c : Fin k → ℝ)
    (h_genic_pos : arch.V_genic c ≠ 0)
    (h_cov_ne : arch.V_cov c ≠ 0) :
    optimalSlopeFromVariance arch c ≠ 1 := by
      exact directionalLD_nonzero_implies_slope_ne_one arch c h_genic_pos h_cov_ne

theorem linear_noise_implies_nonlinear_slope_v2
    (sigma_g_sq base_error slope_error : ℝ)
    (h_g_pos : 0 < sigma_g_sq)
    (hB_pos : 0 < sigma_g_sq + base_error)
    (hB1_pos : 0 < sigma_g_sq + base_error + slope_error)
    (hB2_pos : 0 < sigma_g_sq + base_error + 2 * slope_error)
    (h_slope_ne : slope_error ≠ 0) :
    ∀ (beta0 beta1 : ℝ),
      (fun c => beta0 + beta1 * c) ≠
        (fun c => optimalSlopeLinearNoise sigma_g_sq base_error slope_error c) := by
          exact linear_noise_implies_nonlinear_slope sigma_g_sq base_error slope_error h_g_pos hB_pos hB1_pos hB2_pos h_slope_ne

/-
A bundle of theorems about selection, LD decay, normalization, and drift.
-/
open Calibrator

theorem selection_variation_implies_nonlinear_slope_proven {k : ℕ} [Fintype (Fin k)]
    (arch : GeneticArchitecture k) (c₁ c₂ : Fin k → ℝ)
    (h_genic_pos₁ : arch.V_genic c₁ ≠ 0)
    (h_genic_pos₂ : arch.V_genic c₂ ≠ 0)
    (h_link : ∀ c, arch.selection_effect c = arch.V_cov c / arch.V_genic c)
    (h_sel_var : arch.selection_effect c₁ ≠ arch.selection_effect c₂) :
    optimalSlopeFromVariance arch c₁ ≠ optimalSlopeFromVariance arch c₂ := by
      exact selection_variation_implies_nonlinear_slope arch c₁ c₂ h_genic_pos₁ h_genic_pos₂ h_link h_sel_var

theorem ld_decay_implies_nonlinear_calibration_proven
    (sigma_g_sq base_error slope_error : ℝ)
    (h_g_pos : 0 < sigma_g_sq)
    (h_base : 0 ≤ base_error)
    (h_slope_pos : 0 ≤ slope_error)
    (h_slope_ne : slope_error ≠ 0) :
    ∀ (beta0 beta1 : ℝ),
      (fun c => beta0 + beta1 * c) ≠
        (fun c => optimalSlopeLinearNoise sigma_g_sq base_error slope_error c) := by
          exact ld_decay_implies_nonlinear_calibration sigma_g_sq base_error slope_error h_g_pos h_base h_slope_pos h_slope_ne

theorem normalization_erases_heritability_proven {k : ℕ} [Fintype (Fin k)]
    (arch : GeneticArchitecture k) (c : Fin k → ℝ)
    (h_genic_pos : arch.V_genic c > 0)
    (h_cov_pos : arch.V_cov c > 0) :
    optimalSlopeFromVariance arch c > 1 := by
      exact normalization_erases_heritability arch c h_genic_pos h_cov_pos

theorem neutral_drift_implies_additive_correction_proven {k : ℕ} [Fintype (Fin k)]
    (mech : NeutralScoreDrift k) :
    ∀ c : Fin k → ℝ, driftedScore mech c - mech.drift_artifact c = mech.true_liability := by
      exact neutral_drift_implies_additive_correction mech

theorem confounding_preserves_ranking_proven {k : ℕ} [Fintype (Fin k)]
    (β_env : ℝ) (p1 p2 : ℝ) (c : Fin k → ℝ) (h_le : p1 ≤ p2) :
    p1 + β_env * (∑ l, c l) ≤ p2 + β_env * (∑ l, c l) := by
      linarith

theorem ld_decay_implies_shrinkage_proven {k : ℕ} [Fintype (Fin k)]
    (mech : LDDecayMechanism k) (c_near c_far : Fin k → ℝ)
    (h_dist : mech.distance c_near < mech.distance c_far)
    (h_mono : StrictAnti (mech.tagging_efficiency)) :
    decaySlope mech c_far < decaySlope mech c_near := by
      exact ld_decay_implies_shrinkage mech c_near c_far h_dist h_mono

theorem ld_decay_implies_nonlinear_calibration_sketch_proven {k : ℕ} [Fintype (Fin k)]
    (mech : LDDecayMechanism k)
    (h_nonlin : ¬ ∃ a b, ∀ d ∈ Set.range mech.distance, mech.tagging_efficiency d = a + b * d) :
    ∀ (beta0 beta1 : ℝ),
      (fun c => beta0 + beta1 * mech.distance c) ≠
        (fun c => decaySlope mech c) := by
          exact ld_decay_implies_nonlinear_calibration_sketch mech h_nonlin

theorem optimal_slope_trace_variance_proven {k : ℕ} [Fintype (Fin k)]
    (arch : GeneticArchitecture k) (c : Fin k → ℝ)
    (h_genic_pos : arch.V_genic c ≠ 0) :
    optimalSlopeFromVariance arch c =
      1 + (arch.V_cov c) / (arch.V_genic c) := by
        exact optimal_slope_trace_variance arch c h_genic_pos

theorem normalization_suboptimal_under_ld_proven {k : ℕ} [Fintype (Fin k)]
    (arch : GeneticArchitecture k) (c : Fin k → ℝ)
    (h_genic_pos : arch.V_genic c ≠ 0)
    (h_cov_ne : arch.V_cov c ≠ 0) :
    optimalSlopeFromVariance arch c ≠ 1 := by
      convert directionalLD_nonzero_implies_slope_ne_one_v2 arch c h_genic_pos h_cov_ne using 1

/-
Under independence and zero means, E[P*C] = 0. (With integrability assumptions)
-/
open Calibrator

lemma integral_mul_fst_snd_eq_zero_proven
    (μ : Measure (ℝ × (Fin 1 → ℝ))) [IsProbabilityMeasure μ]
    (h_indep : μ = (μ.map Prod.fst).prod (μ.map Prod.snd))
    (hP_int : Integrable (fun pc => pc.1) μ)
    (hC_int : Integrable (fun pc => pc.2 ⟨0, by norm_num⟩) μ)
    (hP0 : ∫ pc, pc.1 ∂μ = 0)
    (hC0 : ∫ pc, pc.2 ⟨0, by norm_num⟩ ∂μ = 0) :
    ∫ pc, pc.1 * pc.2 ⟨0, by norm_num⟩ ∂μ = 0 := by
      exact integral_mul_fst_snd_eq_zero μ h_indep hP0 hC0

/-
Under independence and zero means, E[P*C] = 0.
-/
open Calibrator

lemma integral_mul_fst_snd_eq_zero_proven_v2
    (μ : Measure (ℝ × (Fin 1 → ℝ))) [IsProbabilityMeasure μ]
    (h_indep : μ = (μ.map Prod.fst).prod (μ.map Prod.snd))
    (hP_int : Integrable (fun pc => pc.1) μ)
    (hC_int : Integrable (fun pc => pc.2 ⟨0, by norm_num⟩) μ)
    (hP0 : ∫ pc, pc.1 ∂μ = 0)
    (hC0 : ∫ pc, pc.2 ⟨0, by norm_num⟩ ∂μ = 0) :
    ∫ pc, pc.1 * pc.2 ⟨0, by norm_num⟩ ∂μ = 0 := by
      exact integral_mul_fst_snd_eq_zero μ h_indep hP0 hC0

/-
Under independence and zero means, E[P*C] = 0.
-/
open Calibrator

lemma integral_mul_fst_snd_eq_zero_proven_v3
    (μ : Measure (ℝ × (Fin 1 → ℝ))) [IsProbabilityMeasure μ]
    (h_indep : μ = (μ.map Prod.fst).prod (μ.map Prod.snd))
    (hP_int : Integrable (fun pc => pc.1) μ)
    (hC_int : Integrable (fun pc => pc.2 ⟨0, by norm_num⟩) μ)
    (hP0 : ∫ pc, pc.1 ∂μ = 0)
    (hC0 : ∫ pc, pc.2 ⟨0, by norm_num⟩ ∂μ = 0) :
    ∫ pc, pc.1 * pc.2 ⟨0, by norm_num⟩ ∂μ = 0 := by
      rw [ integral_mul_fst_snd_eq_zero_proven_v2 ];
      · exact h_indep;
      · exact hP_int;
      · exact hC_int;
      · exact hP0;
      · exact hC0

/-
Under independence and zero means, {1, P, C} are orthogonal in L2.
-/
open Calibrator

lemma orthogonal_features_proven
    (μ : Measure (ℝ × (Fin 1 → ℝ))) [IsProbabilityMeasure μ]
    (h_indep : μ = (μ.map Prod.fst).prod (μ.map Prod.snd))
    (hP_int : Integrable (fun pc => pc.1) μ)
    (hC_int : Integrable (fun pc => pc.2 ⟨0, by norm_num⟩) μ)
    (hP0 : ∫ pc, pc.1 ∂μ = 0)
    (hC0 : ∫ pc, pc.2 ⟨0, by norm_num⟩ ∂μ = 0) :
    (∫ pc, 1 * pc.1 ∂μ = 0) ∧
    (∫ pc, 1 * pc.2 ⟨0, by norm_num⟩ ∂μ = 0) ∧
    (∫ pc, pc.1 * pc.2 ⟨0, by norm_num⟩ ∂μ = 0) := by
  constructor
  · simp; exact hP0
  · constructor
    · simp; exact hC0
    · apply integral_mul_fst_snd_eq_zero_proven_v3 μ h_indep hP_int hC_int hP0 hC0

/-
If a quadratic a*ε + b*ε^2 is non-negative for all ε, then the linear coefficient a must be zero.
-/
lemma linear_coeff_zero_of_quadratic_nonneg_proven (a b : ℝ)
    (h : ∀ ε : ℝ, a * ε + b * ε^2 ≥ 0) : a = 0 := by
      exact linear_coeff_zero_of_quadratic_nonneg a b h

/-
Algebraic solution for optimal coefficients in the additive case.
-/
open Calibrator

lemma optimal_coeffs_raw_additive_standalone_proven
    (a b β_env : ℝ)
    (h_orth_1 : a + b * 0 = 0 + β_env * 0) -- derived from E[resid] = 0
    (h_orth_P : a * 0 + b * 1 = 1 + β_env * 0) -- derived from E[resid*P] = 0
    : a = 0 ∧ b = 1 := by
  constructor
  · simp at h_orth_1
    exact h_orth_1
  · simp at h_orth_P
    exact h_orth_P

/-
Under independence and zero means, E[P*C] = 0.
-/
open Calibrator

lemma integral_mul_fst_snd_eq_zero_final
    (μ : Measure (ℝ × (Fin 1 → ℝ))) [IsProbabilityMeasure μ]
    (h_indep : μ = (μ.map Prod.fst).prod (μ.map Prod.snd))
    (hP_int : Integrable (fun pc => pc.1) μ)
    (hC_int : Integrable (fun pc => pc.2 ⟨0, by norm_num⟩) μ)
    (hP0 : ∫ pc, pc.1 ∂μ = 0)
    (hC0 : ∫ pc, pc.2 ⟨0, by norm_num⟩ ∂μ = 0) :
    ∫ pc, pc.1 * pc.2 ⟨0, by norm_num⟩ ∂μ = 0 := by
      convert integral_mul_fst_snd_eq_zero_proven μ _ _ _ _ _ using 1;
      · exact h_indep;
      · exact hP_int;
      · exact hC_int;
      · exact hP0;
      · exact hC0

/-
If a quadratic a*ε + b*ε^2 is non-negative for all ε, then the linear coefficient a must be zero.
-/
lemma linear_coeff_zero_of_quadratic_nonneg_final (a b : ℝ)
    (h : ∀ ε : ℝ, a * ε + b * ε^2 ≥ 0) : a = 0 := by
      exact linear_coeff_zero_of_quadratic_nonneg a b h

/-
The optimal intercept is the mean of Y when P has zero mean.
-/
open Calibrator

lemma optimal_intercept_eq_mean_of_zero_mean_p_proven
    (μ : Measure (ℝ × (Fin 1 → ℝ))) [IsProbabilityMeasure μ]
    (Y : (ℝ × (Fin 1 → ℝ)) → ℝ) (a b : ℝ)
    (hY : Integrable Y μ)
    (hP : Integrable (fun pc : ℝ × (Fin 1 → ℝ) => pc.1) μ)
    (hP0 : ∫ pc, pc.1 ∂μ = 0)
    (h_orth_1 : ∫ pc, (Y pc - (a + b * pc.1)) ∂μ = 0) :
    a = ∫ pc, Y pc ∂μ := by
      exact optimal_intercept_eq_mean_of_zero_mean_p μ Y a b hY hP hP0 h_orth_1

/-
The optimal slope is the covariance of Y and P when P is normalized (mean 0, variance 1).
-/
open Calibrator

lemma optimal_slope_eq_covariance_of_normalized_p_proven
    (μ : Measure (ℝ × (Fin 1 → ℝ))) [IsProbabilityMeasure μ]
    (Y : (ℝ × (Fin 1 → ℝ)) → ℝ) (a b : ℝ)
    (_hY : Integrable Y μ)
    (hP : Integrable (fun pc : ℝ × (Fin 1 → ℝ) => pc.1) μ)
    (hYP : Integrable (fun pc : ℝ × (Fin 1 → ℝ) => Y pc * pc.1) μ)
    (hP2i : Integrable (fun pc : ℝ × (Fin 1 → ℝ) => pc.1 ^ 2) μ)
    (hP0 : ∫ pc, pc.1 ∂μ = 0)
    (hP2 : ∫ pc, pc.1^2 ∂μ = 1)
    (h_orth_P : ∫ pc, (Y pc - (a + b * pc.1)) * pc.1 ∂μ = 0) :
    b = ∫ pc, Y pc * pc.1 ∂μ := by
  have h_sub : (fun pc => (Y pc - (a + b * pc.1)) * pc.1) = (fun pc => Y pc * pc.1 - a * pc.1 - b * pc.1^2) := by
    ext pc
    ring
  rw [h_sub] at h_orth_P
  rw [integral_sub] at h_orth_P
  · rw [integral_sub] at h_orth_P
    · rw [integral_mul_left, hP0] at h_orth_P
      rw [integral_mul_left, hP2] at h_orth_P
      simp at h_orth_P
      linarith
    · exact hYP
    · apply Integrable.const_mul hP
  · apply Integrable.sub
    · exact hYP
    · apply Integrable.const_mul hP
  · apply Integrable.const_mul hP2i

/-
For a raw score model, the spline terms are zero, so the linear predictor is just `γ₀₀ + γₘ₀ * p`.
-/
open Calibrator

lemma evalSmooth_eq_zero_of_raw_gen_proven {k sp : ℕ} [Fintype (Fin k)] [Fintype (Fin sp)]
    {model : PhenotypeInformedGAM 1 k sp} (h_raw : IsRawScoreModel model)
    (l : Fin k) (c_val : ℝ) :
    evalSmooth model.pcSplineBasis (model.f₀ₗ l) c_val = 0 := by
      exact evalSmooth_eq_zero_of_raw_gen h_raw l c_val

lemma evalSmooth_interaction_eq_zero_of_raw_gen_proven {k sp : ℕ} [Fintype (Fin k)] [Fintype (Fin sp)]
    {model : PhenotypeInformedGAM 1 k sp} (h_raw : IsRawScoreModel model)
    (m : Fin 1) (l : Fin k) (c_val : ℝ) :
    evalSmooth model.pcSplineBasis (model.fₘₗ m l) c_val = 0 := by
      exact evalSmooth_interaction_eq_zero_of_raw_gen h_raw m l c_val

lemma linearPredictor_eq_affine_of_raw_gen_proven {k sp : ℕ} [Fintype (Fin k)] [Fintype (Fin sp)]
    (model_raw : PhenotypeInformedGAM 1 k sp)
    (h_raw : IsRawScoreModel model_raw)
    (h_lin : model_raw.pgsBasis.B ⟨1, by norm_num⟩ = id) :
    ∀ p c, linearPredictor model_raw p c =
      model_raw.γ₀₀ + model_raw.γₘ₀ 0 * p := by
        exact fun p c => linearPredictor_eq_affine_of_raw_gen model_raw h_raw h_lin p c

/-
Bayes-optimality in the raw class implies the residual is orthogonal to 1 and P.
-/
open Calibrator

lemma rawOptimal_implies_orthogonality_gen_proven {k sp : ℕ} [Fintype (Fin k)] [Fintype (Fin sp)]
    (model : PhenotypeInformedGAM 1 k sp) (dgp : DataGeneratingProcess k)
    (h_opt : IsBayesOptimalInRawClass dgp model)
    (h_linear : model.pgsBasis.B ⟨1, by norm_num⟩ = id)
    (hY_int : Integrable (fun pc => dgp.trueExpectation pc.1 pc.2) dgp.jointMeasure)
    (hP_int : Integrable (fun pc => pc.1) dgp.jointMeasure)
    (hP2_int : Integrable (fun pc => pc.1 ^ 2) dgp.jointMeasure)
    (hYP_int : Integrable (fun pc => dgp.trueExpectation pc.1 pc.2 * pc.1) dgp.jointMeasure)
    (h_resid_sq_int : Integrable (fun pc => (dgp.trueExpectation pc.1 pc.2 - (model.γ₀₀ + model.γₘ₀ ⟨0, by norm_num⟩ * pc.1))^2) dgp.jointMeasure) :
    let a := model.γ₀₀
    let b := model.γₘ₀ ⟨0, by norm_num⟩
    (∫ pc, (dgp.trueExpectation pc.1 pc.2 - (a + b * pc.1)) ∂dgp.jointMeasure = 0) ∧
    (∫ pc, (dgp.trueExpectation pc.1 pc.2 - (a + b * pc.1)) * pc.1 ∂dgp.jointMeasure = 0) := by
      exact rawOptimal_implies_orthogonality_gen model dgp h_opt h_linear hY_int hP_int hP2_int hYP_int h_resid_sq_int

/-
The difference in expected squared error when shifting by a constant c is `-2c * E[f] + c^2`.
-/
open Calibrator

lemma integral_square_diff_const {α : Type*} [MeasureSpace α]
    (μ : Measure α) [IsProbabilityMeasure μ]
    (f : α → ℝ) (c : ℝ)
    (hf : Integrable f μ)
    (hf_sq : Integrable (fun x => (f x)^2) μ) :
    ∫ x, (f x - c)^2 ∂μ - ∫ x, (f x)^2 ∂μ = -2 * c * ∫ x, f x ∂μ + c^2 := by
      simp +decide only [sub_sq, mul_assoc, ← integral_const_mul];
      rw [ MeasureTheory.integral_add, MeasureTheory.integral_sub ] <;> norm_num [ hf, hf_sq, mul_assoc, mul_comm c ];
      · rw [ MeasureTheory.integral_neg, MeasureTheory.integral_const_mul ] ; ring;
      · exact hf.mul_const _ |> Integrable.const_mul <| 2;
      · exact hf_sq.sub ( by exact hf.mul_const c |> Integrable.const_mul <| 2 )

/-
If shifting a predictor by any constant ε does not improve the mean squared error, then the mean residual is zero.
-/
open Calibrator

lemma optimal_constant_shift_implies_mean_resid_zero_proven
    {Ω : Type*} [MeasureSpace Ω] {μ : Measure Ω} [IsProbabilityMeasure μ]
    (Y pred : Ω → ℝ)
    (h_resid_sq : Integrable (fun x => (Y x - pred x)^2) μ)
    (h_resid : Integrable (fun x => Y x - pred x) μ)
    (h_opt : ∀ ε : ℝ, ∫ x, (Y x - (pred x + ε))^2 ∂μ ≥ ∫ x, (Y x - pred x)^2 ∂μ) :
    ∫ x, Y x - pred x ∂μ = 0 := by
      have h_expand : ∀ ε : ℝ, (∫ x, (Y x - (pred x + ε))^2 ∂μ) = (∫ x, (Y x - pred x)^2 ∂μ) - 2 * ε * (∫ x, (Y x - pred x) ∂μ) + ε^2 := by
        intro ε
        have h_expand : ∫ x, (Y x - (pred x + ε))^2 ∂μ = ∫ x, (Y x - pred x)^2 ∂μ - 2 * ε * ∫ x, (Y x - pred x) ∂μ + ε^2 * ∫ x, (1 : ℝ) ∂μ := by
          rw [ ← MeasureTheory.integral_const_mul, ← MeasureTheory.integral_const_mul ];
          rw [ ← MeasureTheory.integral_sub, ← MeasureTheory.integral_add ] ; congr ; ext ; ring;
          · exact h_resid_sq.sub ( h_resid.const_mul _ );
          · exact MeasureTheory.integrable_const _;
          · exact h_resid_sq;
          · exact h_resid.const_mul _;
        aesop;
      by_contra h_contra;
      -- Apply the quadratic inequality to conclude that the linear coefficient must be zero.
      have h_linear_coeff_zero : ∀ ε : ℝ, -2 * ε * (∫ x, Y x - pred x ∂μ) + ε^2 ≥ 0 := by
        exact fun ε => by linarith [ h_opt ε, h_expand ε ] ;
      exact h_contra ( by nlinarith [ h_linear_coeff_zero ( ∫ x, Y x - pred x ∂μ ), h_linear_coeff_zero ( - ( ∫ x, Y x - pred x ∂μ ) ) ] )

/-
For the additive bias DGP, the optimal raw coefficients are a=0 and b=1.
-/
open Calibrator

lemma optimal_coefficients_for_additive_dgp_proven
    (model : PhenotypeInformedGAM 1 1 1) (β_env : ℝ)
    (dgp : DataGeneratingProcess 1)
    (h_dgp : dgp.trueExpectation = fun p c => p + β_env * c ⟨0, by norm_num⟩)
    (h_opt : IsBayesOptimalInRawClass dgp model)
    (h_linear : model.pgsBasis.B ⟨1, by norm_num⟩ = id)
    (h_indep : dgp.jointMeasure = (dgp.jointMeasure.map Prod.fst).prod (dgp.jointMeasure.map Prod.snd))
    (hP0 : ∫ pc, pc.1 ∂dgp.jointMeasure = 0)
    (hC0 : ∫ pc, pc.2 ⟨0, by norm_num⟩ ∂dgp.jointMeasure = 0)
    (hP2 : ∫ pc, pc.1^2 ∂dgp.jointMeasure = 1)
    -- Integrability hypotheses
    (hP_int : Integrable (fun pc : ℝ × (Fin 1 → ℝ) => pc.1) dgp.jointMeasure)
    (hC_int : Integrable (fun pc : ℝ × (Fin 1 → ℝ) => pc.2 ⟨0, by norm_num⟩) dgp.jointMeasure)
    (hP2_int : Integrable (fun pc : ℝ × (Fin 1 → ℝ) => pc.1 ^ 2) dgp.jointMeasure)
    (hPC_int : Integrable (fun pc : ℝ × (Fin 1 → ℝ) => pc.2 ⟨0, by norm_num⟩ * pc.1) dgp.jointMeasure)
    (hY_int : Integrable (fun pc : ℝ × (Fin 1 → ℝ) => dgp.trueExpectation pc.1 pc.2) dgp.jointMeasure)
    (hYP_int : Integrable (fun pc : ℝ × (Fin 1 → ℝ) => dgp.trueExpectation pc.1 pc.2 * pc.1) dgp.jointMeasure)
    (h_resid_sq_int : Integrable (fun pc => (dgp.trueExpectation pc.1 pc.2 - (model.γ₀₀ + model.γₘ₀ ⟨0, by norm_num⟩ * pc.1))^2) dgp.jointMeasure) :
    model.γ₀₀ = 0 ∧ model.γₘ₀ ⟨0, by norm_num⟩ = 1 := by
      have h_linear_coeff : (∫ pc : ℝ × (Fin 1 → ℝ), (dgp.trueExpectation pc.1 pc.2 - (model.γ₀₀ + model.γₘ₀ ⟨0, by norm_num⟩ * pc.1)) * pc.1 ∂dgp.jointMeasure) = 0 := by
        convert rawOptimal_implies_orthogonality_gen_proven model dgp h_opt h_linear hY_int hP_int hP2_int hYP_int h_resid_sq_int |>.2 using 1;
      have h_integral_prod : ∫ pc : ℝ × (Fin 1 → ℝ), pc.2 ⟨0, by norm_num⟩ * pc.1 ∂dgp.jointMeasure = (∫ pc : ℝ × (Fin 1 → ℝ), pc.1 ∂dgp.jointMeasure) * (∫ pc : ℝ × (Fin 1 → ℝ), pc.2 ⟨0, by norm_num⟩ ∂dgp.jointMeasure) := by
        rw [ h_indep, MeasureTheory.integral_prod ];
        · simp +decide [ mul_comm, MeasureTheory.integral_mul_const, MeasureTheory.integral_const_mul, MeasureTheory.integral_prod ];
          rw [ MeasureTheory.integral_map, MeasureTheory.integral_map ];
          · rw [ ← h_indep ];
          · exact measurable_snd.aemeasurable;
          · exact measurable_pi_apply 0 |> Measurable.aestronglyMeasurable;
          · exact measurable_fst.aemeasurable;
          · exact measurable_id.aestronglyMeasurable;
        · convert hPC_int using 1;
          exact h_indep.symm;
      have h_linear_coeff : (∫ pc : ℝ × (Fin 1 → ℝ), (dgp.trueExpectation pc.1 pc.2 - (model.γ₀₀ + model.γₘ₀ ⟨0, by norm_num⟩ * pc.1)) * pc.1 ∂dgp.jointMeasure) = (∫ pc : ℝ × (Fin 1 → ℝ), pc.1^2 ∂dgp.jointMeasure) - model.γₘ₀ ⟨0, by norm_num⟩ * (∫ pc : ℝ × (Fin 1 → ℝ), pc.1^2 ∂dgp.jointMeasure) := by
        simp +decide [ h_dgp, sub_mul, add_mul, mul_assoc, mul_comm, mul_left_comm, sq, MeasureTheory.integral_const_mul, MeasureTheory.integral_mul_const ];
        simp +decide [ mul_add, mul_sub, mul_assoc, mul_comm, mul_left_comm, ← MeasureTheory.integral_const_mul ];
        rw [ MeasureTheory.integral_sub, MeasureTheory.integral_add ];
        · rw [ MeasureTheory.integral_add ];
          · simp +decide [ mul_assoc, MeasureTheory.integral_const_mul, MeasureTheory.integral_mul_const, hP0, hC0, h_integral_prod ];
            exact Or.inr ( by simpa only [ mul_comm ] using h_integral_prod.trans ( by simp +decide [ hP0, hC0 ] ) );
          · exact hP_int.mul_const _;
          · convert hP2_int.mul_const ( model.γₘ₀ ⟨ 0, by norm_num ⟩ ) using 2 ; ring;
            rfl;
        · simpa only [ sq ] using hP2_int;
        · exact MeasureTheory.Integrable.const_mul ( by simpa only [ mul_comm ] using hPC_int ) _;
        · refine' MeasureTheory.Integrable.add _ _;
          · simpa only [ sq ] using hP2_int;
          · exact MeasureTheory.Integrable.const_mul ( by simpa only [ mul_comm ] using hPC_int ) _;
        · ring_nf;
          exact MeasureTheory.Integrable.add ( hP_int.mul_const _ ) ( hP2_int.mul_const _ );
      have h_linear_coeff : (∫ pc : ℝ × (Fin 1 → ℝ), dgp.trueExpectation pc.1 pc.2 - (model.γ₀₀ + model.γₘ₀ ⟨0, by norm_num⟩ * pc.1) ∂dgp.jointMeasure) = 0 := by
        convert rawOptimal_implies_orthogonality_gen_proven model dgp h_opt h_linear hY_int hP_int hP2_int hYP_int h_resid_sq_int |>.1 using 1;
      rw [ MeasureTheory.integral_sub ] at h_linear_coeff;
      · rw [ MeasureTheory.integral_add ] at h_linear_coeff <;> norm_num at *;
        · rw [ MeasureTheory.integral_const_mul ] at h_linear_coeff ; norm_num [ hP0, hC0, hP2 ] at h_linear_coeff;
          rw [ h_dgp ] at h_linear_coeff;
          rw [ MeasureTheory.integral_add ] at h_linear_coeff <;> norm_num at *;
          · rw [ MeasureTheory.integral_const_mul ] at h_linear_coeff ; norm_num [ hP0, hC0, hP2 ] at h_linear_coeff ; constructor <;> nlinarith;
          · exact hP_int;
          · exact hC_int.const_mul _;
        · exact hP_int.const_mul _;
      · exact hY_int;
      · exact MeasureTheory.Integrable.add ( MeasureTheory.integrable_const _ ) ( MeasureTheory.Integrable.const_mul hP_int _ )

/-
L2 integrability implies L1 integrability on a finite measure space.
-/
open MeasureTheory

lemma integrable_of_integrable_sq_proven {α : Type*} [MeasureSpace α] {μ : Measure α} [IsFiniteMeasure μ]
    {f : α → ℝ} (hf_meas : AEStronglyMeasurable f μ)
    (hf_sq : Integrable (fun x => (f x)^2) μ) :
    Integrable f μ := by
      refine' MeasureTheory.Integrable.mono' _ _ _;
      exacts [ fun x => f x ^ 2 + 1, by exact MeasureTheory.Integrable.add hf_sq ( MeasureTheory.integrable_const _ ), hf_meas, Filter.Eventually.of_forall fun x => by rw [ Real.norm_eq_abs, abs_le ] ; constructor <;> nlinarith ]
