diff --git a/calibrate/basis.rs b/calibrate/basis.rs
index 2632eee..43a007d 100644
--- a/calibrate/basis.rs
+++ b/calibrate/basis.rs
@@ -1,4 +1,5 @@
 use ndarray::{Array, Array1, Array2, ArrayView1, ArrayView2, Axis, s};
+use ndarray_linalg::{Eigh, UPLO};
 
 use serde::{Deserialize, Serialize};
 use thiserror::Error;
@@ -270,6 +271,64 @@ pub fn apply_sum_to_zero_constraint(
     Ok((constrained, z))
 }
 
+/// Decomposes a penalty matrix S into its null-space and whitened range-space components.
+/// This is used for functional ANOVA decomposition in GAMs to separate unpenalized 
+/// and penalized subspaces of a basis.
+///
+/// # Arguments
+/// * `s_1d`: The 1D penalty matrix (typically a difference penalty matrix)
+/// * `tol`: Tolerance for determining zero eigenvalues (typically 1e-12)
+///
+/// # Returns
+/// A tuple of transformation matrices: (Z_null, Z_range_whiten) where:
+/// - `Z_null`: Orthogonal basis for the null space (unpenalized functions)
+/// - `Z_range_whiten`: Whitened basis for the range space (penalized functions)
+///   In these coordinates, the penalty becomes an identity matrix.
+pub fn null_range_whiten(
+    s_1d: &Array2<f64>,
+    tol: f64,
+) -> Result<(Array2<f64>, Array2<f64>), BasisError> {
+    let (evals, evecs) = s_1d
+        .eigh(UPLO::Lower)
+        .map_err(BasisError::LinalgError)?;
+
+    let mut idx_n = Vec::new();
+    let mut idx_r = Vec::new();
+    for (i, &d) in evals.iter().enumerate() {
+        if d.abs() <= tol {
+            idx_n.push(i);
+        } else {
+            idx_r.push(i);
+        }
+    }
+
+    // Build basis for the null space (unpenalized part)
+    let z_null = select_columns(&evecs, &idx_n);
+
+    // Build whitened basis for the range space (penalized part)
+    let mut d_inv_sqrt = Array2::<f64>::zeros((idx_r.len(), idx_r.len()));
+    for (j, &i) in idx_r.iter().enumerate() {
+        d_inv_sqrt[[j, j]] = 1.0 / evals[i].sqrt();
+    }
+    let z_range_whiten = select_columns(&evecs, &idx_r).dot(&d_inv_sqrt);
+
+    Ok((z_null, z_range_whiten))
+}
+
+/// Helper function to select specific columns from a matrix by index.
+/// This is needed because ndarray doesn't have a direct way to select non-contiguous columns.
+fn select_columns(matrix: &Array2<f64>, indices: &[usize]) -> Array2<f64> {
+    let nrows = matrix.nrows();
+    let ncols = indices.len();
+    let mut result = Array2::zeros((nrows, ncols));
+    
+    for (j, &col_idx) in indices.iter().enumerate() {
+        result.column_mut(j).assign(&matrix.column(col_idx));
+    }
+    
+    result
+}
+
 /// Internal module for implementation details not exposed in the public API.
 mod internal {
     use super::*;
diff --git a/calibrate/construction.rs b/calibrate/construction.rs
index 0a69f65..1de145f 100644
--- a/calibrate/construction.rs
+++ b/calibrate/construction.rs
@@ -10,6 +10,8 @@ use std::ops::Range;
 /// Computes the Kronecker product A ⊗ B for penalty matrix construction.
 /// This is used to create tensor product penalties that enforce smoothness
 /// in multiple dimensions for interaction terms.
+/// NOTE: Currently unused due to penalty grouping in Option 3 implementation
+#[allow(dead_code)]
 fn kronecker_product(a: &Array2<f64>, b: &Array2<f64>) -> Array2<f64> {
     let (a_rows, a_cols) = a.dim();
     let (b_rows, b_cols) = b.dim();
@@ -100,7 +102,7 @@ impl ModelLayout {
     ) -> Result<Self, EstimationError> {
         let mut penalty_map = Vec::new();
         let mut current_col = 0;
-        let mut penalty_idx_counter = 0;
+        let mut _penalty_idx_counter = 0;
 
         // Calculate total coefficients first to ensure consistency
         // Formula: total_coeffs = 1 (intercept) + p_pgs_main + p_pc_main + p_interactions
@@ -117,17 +119,16 @@ impl ModelLayout {
         let intercept_col = current_col;
         current_col += 1;
 
-        // Main effect for each PC
+        // Main effect for each PC (all share penalty index 0 due to grouping)
         for (i, &num_basis) in pc_constrained_basis_ncols.iter().enumerate() {
             let range = current_col..current_col + num_basis;
             penalty_map.push(PenalizedBlock {
                 term_name: format!("f({})", config.pc_names[i]),
                 col_range: range.clone(),
-                penalty_indices: vec![penalty_idx_counter],
+                penalty_indices: vec![0], // All PC mains share penalty index 0
                 term_type: TermType::PcMainEffect,
             });
             current_col += num_basis;
-            penalty_idx_counter += 1;
         }
 
         // Main effect for PGS (non-constant basis terms)
@@ -135,30 +136,34 @@ impl ModelLayout {
         let pgs_main_cols = current_col..current_col + pgs_main_basis_ncols;
         current_col += pgs_main_basis_ncols; // Still advance the column counter
 
-        // Tensor product interaction effects (conditionally)
-        // Each PC gets one tensor product interaction block with dual penalties
+        // Tensor product interaction effects (all share penalty index 1 due to grouping)
         if num_pgs_interaction_bases > 0 {
             for (i, &num_pc_basis_unc) in pc_unconstrained_basis_ncols.iter().enumerate() {
                 let num_tensor_coeffs = num_pgs_interaction_bases * num_pc_basis_unc;
                 let range = current_col..current_col + num_tensor_coeffs;
 
-                // Create tensor product block with two penalty indices
-                // One for PGS direction smoothness, one for PC direction smoothness
-                let pgs_penalty_idx = penalty_idx_counter;
-                let pc_penalty_idx = penalty_idx_counter + 1;
-
                 penalty_map.push(PenalizedBlock {
                     term_name: format!("f(PGS,{})", config.pc_names[i]),
                     col_range: range.clone(),
-                    penalty_indices: vec![pgs_penalty_idx, pc_penalty_idx],
+                    penalty_indices: vec![1], // All interactions share penalty index 1
                     term_type: TermType::Interaction,
                 });
 
                 current_col += num_tensor_coeffs;
-                penalty_idx_counter += 2; // Two penalties per tensor product interaction
             }
         }
 
+        // With penalty grouping, we have at most 2 penalties: 
+        // 0 for PC mains (if any), 1 for interactions (if any)
+        let has_pc_mains = !pc_constrained_basis_ncols.is_empty();
+        let has_interactions = num_pgs_interaction_bases > 0 && !pc_constrained_basis_ncols.is_empty();
+        _penalty_idx_counter = match (has_pc_mains, has_interactions) {
+            (true, true) => 2,   // Both PC mains and interactions
+            (true, false) => 1,  // Only PC mains
+            (false, true) => 1,  // Only interactions (shouldn't happen in practice)
+            (false, false) => 0, // No penalties
+        };
+
         // Verify that our calculation matches the actual column count
         if current_col != calculated_total_coeffs {
             return Err(EstimationError::LayoutError(format!(
@@ -171,13 +176,13 @@ impl ModelLayout {
             pgs_main_cols,
             penalty_map,
             total_coeffs: current_col,
-            num_penalties: penalty_idx_counter,
+            num_penalties: _penalty_idx_counter,
         })
     }
 }
 
 /// Constructs the design matrix `X` and a list of individual penalty matrices `S_i`.
-/// Returns the design matrix, penalty matrices, model layout, constraint transformations, knot vectors, and PGS basis means.
+/// Returns the design matrix, penalty matrices, model layout, constraint transformations, knot vectors, and range transformations.
 pub fn build_design_and_penalty_matrices(
     data: &TrainingData,
     config: &ModelConfig,
@@ -188,6 +193,7 @@ pub fn build_design_and_penalty_matrices(
         ModelLayout,
         HashMap<String, Constraint>,
         HashMap<String, Array1<f64>>,
+        HashMap<String, Array2<f64>>,
     ),
     EstimationError,
 > {
@@ -203,9 +209,10 @@ pub fn build_design_and_penalty_matrices(
 
     let n_samples = data.y.len();
 
-    // Initialize constraint and knot vector storage
+    // Initialize constraint, knot vector, and range transform storage
     let mut constraints = HashMap::new();
     let mut knot_vectors = HashMap::new();
+    let mut range_transforms = HashMap::new();
 
     // 1. Generate basis for PGS and apply sum-to-zero constraint
     let (pgs_basis_unc, pgs_knots) = create_bspline_basis(
@@ -219,13 +226,18 @@ pub fn build_design_and_penalty_matrices(
     // Save PGS knot vector
     knot_vectors.insert("pgs".to_string(), pgs_knots);
 
-    // Apply sum-to-zero constraint to PGS main effects (excluding intercept)
+    // Create PGS penalty matrix and decompose into null/range spaces
     let pgs_main_basis_unc = pgs_basis_unc.slice(s![.., 1..]);
+    let s_pgs_base = create_difference_penalty_matrix(pgs_main_basis_unc.ncols(), config.penalty_order)?;
+    let (_z_null_pgs, z_range_pgs) = basis::null_range_whiten(&s_pgs_base, 1e-12)?;
+    
+    // For policy mode: PGS main effect remains unpenalized (use full unconstrained basis)
+    // Apply sum-to-zero constraint to maintain identifiability
     let (pgs_main_basis, pgs_z_transform) =
         basis::apply_sum_to_zero_constraint(pgs_main_basis_unc)?;
 
-    // CRITICAL: For interactions, use the UNCONSTRAINED pgs_main_basis_unc for the tensor product
-    // to match the training logic and ensure dimensional consistency.
+    // Store PGS range transformation for interactions and potential future penalized PGS
+    range_transforms.insert("pgs".to_string(), z_range_pgs);
 
     // Save the PGS constraint transformation
     constraints.insert(
@@ -235,10 +247,9 @@ pub fn build_design_and_penalty_matrices(
         },
     );
 
-    // 2. Generate constrained bases and unscaled penalty matrices for PCs
-    let mut pc_constrained_bases = Vec::new();
+    // 2. Generate range-only bases for PCs (functional ANOVA decomposition)
+    let mut pc_range_bases = Vec::new();
     let mut pc_unconstrained_bases_main = Vec::new();
-    let mut s_list = Vec::new();
 
     // Check if we have any PCs to process
     if config.pc_names.is_empty() {
@@ -258,144 +269,84 @@ pub fn build_design_and_penalty_matrices(
 
         // Save PC knot vector
         knot_vectors.insert(pc_name.clone(), pc_knots);
-        // Apply sum-to-zero constraint to PC main effects (excluding intercept)
+        
+        // Functional ANOVA decomposition: use range-only basis for PC main effects
         let pc_main_basis_unc = pc_basis_unc.slice(s![.., 1..]);
         pc_unconstrained_bases_main.push(pc_main_basis_unc.to_owned());
-        let (constrained_basis, z_transform) =
-            basis::apply_sum_to_zero_constraint(pc_main_basis_unc)?;
-        pc_constrained_bases.push(constrained_basis);
-
-        // Save the PC constraint transformation
-        let pc_name = &config.pc_names[i];
+        
+        // Create penalty matrix and decompose into null/range spaces
+        let s_pc_base = create_difference_penalty_matrix(pc_main_basis_unc.ncols(), config.penalty_order)?;
+        let (_z_null_pc, z_range_pc) = basis::null_range_whiten(&s_pc_base, 1e-12)?;
+        
+        // PC main effect uses ONLY the range (penalized) part
+        let pc_range_basis = pc_main_basis_unc.dot(&z_range_pc);
+        pc_range_bases.push(pc_range_basis);
+        
+        // Store PC range transformation for interactions and main effects
+        range_transforms.insert(pc_name.clone(), z_range_pc.clone());
+        
+        // Store the actual range transformation for backward compatibility
+        // This allows prediction fallback to work correctly when range_transforms is empty
         constraints.insert(
             pc_name.clone(),
             Constraint {
-                z_transform: z_transform.clone(),
+                z_transform: z_range_pc.clone(),
             },
         );
-
-        // Transform the penalty matrix: S_constrained = Z^T * S_unconstrained * Z
-        let s_unconstrained =
-            create_difference_penalty_matrix(pc_main_basis_unc.ncols(), config.penalty_order)?;
-        let s_constrained = z_transform.t().dot(&s_unconstrained.dot(&z_transform));
-
-        // Embed into full-sized p × p matrix (will be determined after layout is created)
-        s_list.push(s_constrained);
-    }
-
-    // 3. Create tensor product penalties for interaction effects (conditionally)
-    // Each PC interaction gets two penalties: one for PGS direction, one for PC direction
-    // Use the number of columns from the UNCONSTRAINED basis for layout planning.
-    let num_pgs_interaction_bases = pgs_main_basis_unc.ncols();
-
-    if num_pgs_interaction_bases > 0 && !pc_unconstrained_bases_main.is_empty() {
-        // Create base penalty matrices for Kronecker products
-        let s_pgs_base =
-            create_difference_penalty_matrix(num_pgs_interaction_bases, config.penalty_order)?;
-
-        for i in 0..pc_unconstrained_bases_main.len() {
-            let num_pc_bases = pc_unconstrained_bases_main[i].ncols();
-            let s_pc_base = create_difference_penalty_matrix(num_pc_bases, config.penalty_order)?;
-
-            // Create identity matrices for Kronecker products
-            let i_pgs = Array2::eye(num_pgs_interaction_bases);
-            let i_pc = Array2::eye(num_pc_bases);
-
-            // Create the two tensor product penalties:
-            // 1. S_pgs ⊗ I_pc (penalizes roughness in PGS direction)
-            let s_pgs_tensor = kronecker_product(&s_pgs_base, &i_pc);
-            s_list.push(s_pgs_tensor);
-
-            // 2. I_pgs ⊗ S_pc (penalizes roughness in PC direction)
-            let s_pc_tensor = kronecker_product(&i_pgs, &s_pc_base);
-            s_list.push(s_pc_tensor);
-        }
     }
 
-    // 4. Define the model layout based on final basis dimensions
-    let pc_constrained_ncols: Vec<usize> = pc_constrained_bases.iter().map(|b| b.ncols()).collect();
-    let pc_unconstrained_ncols: Vec<usize> = pc_unconstrained_bases_main
-        .iter()
-        .map(|b| b.ncols())
-        .collect();
-
-    // Use empirically calculated interaction bases for layout consistency
+    // 3. Penalty grouping setup: Create grouped penalty matrices
+    // This reduces the number of smoothing parameters from O(#PCs) to just 2-3
+    let pc_range_ncols: Vec<usize> = pc_range_bases.iter().map(|b| b.ncols()).collect();
+    let pgs_range_ncols = range_transforms.get("pgs").map(|rt| rt.ncols()).unwrap_or(0);
+    
+    // Calculate layout first to determine matrix dimensions for penalty grouping
     let layout = ModelLayout::new(
         config,
-        &pc_constrained_ncols,
+        &pc_range_ncols,
         pgs_main_basis.ncols(),
-        &pc_unconstrained_ncols,
-        num_pgs_interaction_bases,
+        &pc_range_ncols, // Use range ncols for interactions too
+        pgs_range_ncols, // Use PGS range size for interactions
     )?;
 
-    if s_list.len() != layout.num_penalties {
-        return Err(EstimationError::LayoutError(format!(
-            "Internal logic error: Mismatch in number of penalties. Layout expects {}, but {} were generated.",
-            layout.num_penalties,
-            s_list.len()
-        )));
-    }
-
-    // Embed all penalty matrices into full-sized p × p matrices
-    // This ensures all matrices in s_list are compatible for linear algebra operations
+    // 4. Create grouped penalty matrices with identity penalties in whitened coordinates
     let p = layout.total_coeffs;
-    let mut s_list_full = Vec::new();
-
-    // Validate that s_list length matches the total number of penalty indices in penalty_map
-    let expected_penalty_count: usize = layout
-        .penalty_map
-        .iter()
-        .map(|block| block.penalty_indices.len())
-        .sum();
-    if s_list.len() != expected_penalty_count {
-        return Err(EstimationError::LayoutError(format!(
-            "Penalty matrix count mismatch: s_list has {} matrices but expected {} based on penalty_map",
-            s_list.len(),
-            expected_penalty_count
-        )));
-    }
-
-    for (k, s_k) in s_list.iter().enumerate() {
-        let mut s_k_full = Array2::zeros((p, p));
-
-        // Find the block for this penalty index and embed the matrix
-        let mut found_block = false;
-        for block in &layout.penalty_map {
-            if block.penalty_indices.contains(&k) {
-                let col_range = block.col_range.clone();
-                let block_size = col_range.len();
-
-                // Validate dimensions
-                if s_k.nrows() != block_size || s_k.ncols() != block_size {
-                    return Err(EstimationError::LayoutError(format!(
-                        "Penalty matrix {} (term {}) has size {}×{} but block expects {}×{}",
-                        k,
-                        block.term_name,
-                        s_k.nrows(),
-                        s_k.ncols(),
-                        block_size,
-                        block_size
-                    )));
-                }
-
-                // Embed the block-sized penalty into the full-sized matrix
-                s_k_full
-                    .slice_mut(s![col_range.clone(), col_range])
-                    .assign(s_k);
-                found_block = true;
-                break;
-            }
+    let mut s_pc_main_total = Array2::zeros((p, p));
+    let mut s_inter_total = Array2::zeros((p, p));
+    
+    // Add identity penalties for PC main effects (grouped)
+    for block in &layout.penalty_map {
+        if block.term_type == crate::calibrate::construction::TermType::PcMainEffect {
+            let col_range = block.col_range.clone();
+            let range_len = col_range.len();
+            s_pc_main_total
+                .slice_mut(s![col_range.clone(), col_range])
+                .assign(&Array2::eye(range_len));
         }
-
-        if !found_block {
-            return Err(EstimationError::LayoutError(format!(
-                "No block found for penalty matrix index {k}"
-            )));
+    }
+    
+    // Add identity penalties for interaction effects (grouped)
+    for block in &layout.penalty_map {
+        if block.term_type == crate::calibrate::construction::TermType::Interaction {
+            let col_range = block.col_range.clone();
+            let range_len = col_range.len();
+            s_inter_total
+                .slice_mut(s![col_range.clone(), col_range])
+                .assign(&Array2::eye(range_len));
         }
-
-        s_list_full.push(s_k_full);
+    }
+    
+    // Build the final penalty list (reduced from many to just 2)
+    let mut s_list = Vec::new();
+    if s_pc_main_total.iter().any(|&v: &f64| v.abs() > 0.0) {
+        s_list.push(s_pc_main_total);
+    }
+    if s_inter_total.iter().any(|&v: &f64| v.abs() > 0.0) {
+        s_list.push(s_inter_total);
     }
 
+    // Range transformations will be returned directly to the caller
+
     // 5. Assemble the full design matrix `X` using the layout as the guide
     // Following a strict canonical order to match the coefficient flattening logic in model.rs
     let mut x_matrix = Array2::zeros((n_samples, layout.total_coeffs));
@@ -403,17 +354,17 @@ pub fn build_design_and_penalty_matrices(
     // 1. Intercept - always the first column
     x_matrix.column_mut(layout.intercept_col).fill(1.0);
 
-    // 2. Main PC effects - iterate through PC bases in order of config.pc_names
+    // 2. Main PC effects - use range-only bases (fully penalized)
     for (pc_idx, pc_name) in config.pc_names.iter().enumerate() {
         for block in &layout.penalty_map {
             if block.term_name == format!("f({pc_name})") {
                 let col_range = block.col_range.clone();
-                let pc_basis = &pc_constrained_bases[pc_idx];
+                let pc_basis = &pc_range_bases[pc_idx];
 
                 // Validate dimensions before assignment
                 if pc_basis.nrows() != n_samples {
                     return Err(EstimationError::LayoutError(format!(
-                        "PC basis {} has {} rows but expected {} samples",
+                        "PC range basis {} has {} rows but expected {} samples",
                         pc_name,
                         pc_basis.nrows(),
                         n_samples
@@ -421,7 +372,7 @@ pub fn build_design_and_penalty_matrices(
                 }
                 if pc_basis.ncols() != col_range.len() {
                     return Err(EstimationError::LayoutError(format!(
-                        "PC basis {} has {} columns but layout expects {} columns",
+                        "PC range basis {} has {} columns but layout expects {} columns",
                         pc_name,
                         pc_basis.ncols(),
                         col_range.len()
@@ -457,9 +408,12 @@ pub fn build_design_and_penalty_matrices(
         .slice_mut(s![.., pgs_range])
         .assign(&pgs_main_basis);
 
-    // 4. Tensor product interaction effects (conditionally)
-    // Create one unified interaction surface per PC using proper tensor products
-    if num_pgs_interaction_bases > 0 && !pc_unconstrained_bases_main.is_empty() {
+    // 4. Tensor product interaction effects - Range × Range only (fully penalized)
+    if pgs_range_ncols > 0 && !pc_range_bases.is_empty() {
+        // Get PGS range basis for interactions
+        let pgs_range_transform = range_transforms.get("pgs").unwrap();
+        let pgs_range_basis = pgs_main_basis_unc.dot(pgs_range_transform);
+        
         for (pc_idx, pc_name) in config.pc_names.iter().enumerate() {
             // Find the corresponding tensor product block in the layout
             let tensor_block = layout
@@ -473,17 +427,16 @@ pub fn build_design_and_penalty_matrices(
                     ))
                 })?;
 
-            // Create the tensor product design matrix columns using row-wise tensor product
-            // This replaces the flawed "dimple-maker" approach with proper 2D basis functions
-            let pc_unconstrained_basis = &pc_unconstrained_bases_main[pc_idx];
-            let tensor_interaction =
-                row_wise_tensor_product(&pgs_main_basis_unc.to_owned(), pc_unconstrained_basis);
+            // Create Range × Range tensor product (guarantees full penalization)
+            let pc_range_transform = range_transforms.get(pc_name).unwrap();
+            let pc_range_basis = pc_unconstrained_bases_main[pc_idx].dot(pc_range_transform);
+            let tensor_interaction = row_wise_tensor_product(&pgs_range_basis, &pc_range_basis);
 
             // Validate dimensions
             let col_range = tensor_block.col_range.clone();
             if tensor_interaction.nrows() != n_samples {
                 return Err(EstimationError::LayoutError(format!(
-                    "Tensor interaction f(PGS,{}) has {} rows but expected {} samples",
+                    "Range×Range tensor interaction f(PGS,{}) has {} rows but expected {} samples",
                     pc_name,
                     tensor_interaction.nrows(),
                     n_samples
@@ -491,16 +444,13 @@ pub fn build_design_and_penalty_matrices(
             }
             if tensor_interaction.ncols() != col_range.len() {
                 return Err(EstimationError::LayoutError(format!(
-                    "Tensor interaction f(PGS,{}) has {} columns but layout expects {} columns",
+                    "Range×Range tensor interaction f(PGS,{}) has {} columns but layout expects {} columns",
                     pc_name,
                     tensor_interaction.ncols(),
                     col_range.len()
                 )));
             }
 
-            // No constraint transformation needed for the tensor product
-            // (The constraint for tensor products was unused in prediction code)
-
             // Assign to the design matrix
             x_matrix
                 .slice_mut(s![.., col_range])
@@ -539,7 +489,7 @@ pub fn build_design_and_penalty_matrices(
         });
     }
 
-    Ok((x_matrix, s_list_full, layout, constraints, knot_vectors))
+    Ok((x_matrix, s_list, layout, constraints, knot_vectors, range_transforms))
 }
 
 /// Result of the stable reparameterization algorithm from Wood (2011) Appendix B
@@ -1330,6 +1280,7 @@ mod tests {
             pc_names,
             constraints: HashMap::new(),
             knot_vectors: HashMap::new(),
+            range_transforms: HashMap::new(),
         };
 
         (data, config)
@@ -1340,18 +1291,22 @@ mod tests {
         // Setup with 1 PC to create main effect and interaction terms
         let (data, config) = create_test_data_for_construction(100, 1);
 
-        let (x, s_list, layout, _, _) = build_design_and_penalty_matrices(&data, &config).unwrap();
+        let (x, s_list, layout, _, _, range_transforms) = build_design_and_penalty_matrices(&data, &config).unwrap();
 
-        // Theoretical calculation of dimensions
+        // Option 3 dimensional calculation using range transforms
         let pgs_main_coeffs =
-            config.pgs_basis_config.num_knots + config.pgs_basis_config.degree - 1;
-        let pc_main_coeffs =
-            config.pc_basis_configs[0].num_knots + config.pc_basis_configs[0].degree - 1;
-        let interaction_coeffs = (config.pgs_basis_config.num_knots
-            + config.pgs_basis_config.degree)
-            * (config.pc_basis_configs[0].num_knots + config.pc_basis_configs[0].degree);
+            config.pgs_basis_config.num_knots + config.pgs_basis_config.degree - 1; // Unchanged (sum-to-zero constrained)
+        
+        // PC main coeffs = range dimension (fully penalized)
+        let r_pc = range_transforms.get("PC1").unwrap().ncols();
+        let pc_main_coeffs = r_pc;
+        
+        // Interaction coeffs = r_pgs × r_pc (Range × Range tensor product)
+        let r_pgs = range_transforms.get("pgs").unwrap().ncols();
+        let interaction_coeffs = r_pgs * r_pc;
+        
         let expected_total_coeffs = 1 + pgs_main_coeffs + pc_main_coeffs + interaction_coeffs;
-        let expected_num_penalties = 1 + 2; // 1 for PC main + 2 for interaction
+        let expected_num_penalties = 2; // Grouped: 1 for all PC mains, 1 for all interactions
 
         assert_eq!(
             layout.total_coeffs, expected_total_coeffs,
@@ -1376,7 +1331,7 @@ mod tests {
     #[test]
     fn test_interaction_design_matrix_is_full_rank() {
         let (data, config) = create_test_data_for_construction(100, 1);
-        let (x, _, _, _, _) = build_design_and_penalty_matrices(&data, &config).unwrap();
+        let (x, _, _, _, _, _) = build_design_and_penalty_matrices(&data, &config).unwrap();
 
         // Calculate numerical rank via SVD
         let svd = x.svd(false, false).expect("SVD failed");
@@ -1399,7 +1354,14 @@ mod tests {
     #[test]
     fn test_interaction_term_has_correct_penalty_structure() {
         let (data, config) = create_test_data_for_construction(100, 1);
-        let (_, s_list, layout, _, _) = build_design_and_penalty_matrices(&data, &config).unwrap();
+        let (_, s_list, layout, _, _, _) = build_design_and_penalty_matrices(&data, &config).unwrap();
+
+        // Option 3: Expect grouped penalties - total of 2 penalty matrices
+        assert_eq!(
+            s_list.len(),
+            2,
+            "Option 3 should have exactly 2 grouped penalty matrices"
+        );
 
         let interaction_block = layout
             .penalty_map
@@ -1407,44 +1369,62 @@ mod tests {
             .find(|b| b.term_type == TermType::Interaction)
             .expect("Interaction block not found in layout");
 
+        // Option 3: Each block now has only 1 penalty index (grouped)
         assert_eq!(
             interaction_block.penalty_indices.len(),
-            2,
-            "Interaction term must have two penalties."
+            1,
+            "Interaction term should have one grouped penalty index"
         );
 
-        let pgs_penalty_idx = interaction_block.penalty_indices[0];
-        let pc_penalty_idx = interaction_block.penalty_indices[1];
-
-        let s_pgs_dir = &s_list[pgs_penalty_idx];
-        let s_pc_dir = &s_list[pc_penalty_idx];
-
-        // The two penalty matrices must be different
-        let diff = (s_pgs_dir - s_pc_dir).mapv(|x| x.abs()).sum();
-        assert!(
-            diff > 1e-9,
-            "The two interaction penalties should not be identical."
+        let interaction_penalty_idx = interaction_block.penalty_indices[0];
+        assert_eq!(
+            interaction_penalty_idx,
+            1,
+            "Interaction should use penalty index 1 (PC mains use 0)"
         );
 
-        // Check that penalties are confined to the correct block
-        for s_matrix in [s_pgs_dir, s_pc_dir] {
-            for r in 0..layout.total_coeffs {
-                for c in 0..layout.total_coeffs {
-                    if !interaction_block.col_range.contains(&r)
-                        || !interaction_block.col_range.contains(&c)
-                    {
-                        assert_abs_diff_eq!(s_matrix[[r, c]], 0.0, epsilon = 1e-12);
+        // Verify penalty matrix structure
+        let s_interactions = &s_list[1]; // Interaction penalty matrix
+        let s_pc_mains = &s_list[0];     // PC main effects penalty matrix
+
+        // Check that interaction penalty matrix has identity on interaction blocks and zeros elsewhere
+        for r in 0..layout.total_coeffs {
+            for c in 0..layout.total_coeffs {
+                if interaction_block.col_range.contains(&r) && interaction_block.col_range.contains(&c) {
+                    // Within interaction block: should be identity (1 on diagonal, 0 off-diagonal)
+                    if r == c {
+                        assert_abs_diff_eq!(s_interactions[[r, c]], 1.0, epsilon = 1e-12);
+                    } else {
+                        assert_abs_diff_eq!(s_interactions[[r, c]], 0.0, epsilon = 1e-12);
                     }
+                } else {
+                    // Outside interaction block: should be zero
+                    assert_abs_diff_eq!(s_interactions[[r, c]], 0.0, epsilon = 1e-12);
+                }
+            }
+        }
+
+        // Check that PC main penalty matrix has identity on PC main blocks and zeros elsewhere
+        let pc_main_block = layout
+            .penalty_map
+            .iter()
+            .find(|b| b.term_type == TermType::PcMainEffect)
+            .expect("PC main effect block not found in layout");
+
+        for r in 0..layout.total_coeffs {
+            for c in 0..layout.total_coeffs {
+                if pc_main_block.col_range.contains(&r) && pc_main_block.col_range.contains(&c) {
+                    // Within PC main block: should be identity
+                    if r == c {
+                        assert_abs_diff_eq!(s_pc_mains[[r, c]], 1.0, epsilon = 1e-12);
+                    } else {
+                        assert_abs_diff_eq!(s_pc_mains[[r, c]], 0.0, epsilon = 1e-12);
+                    }
+                } else {
+                    // Outside PC main block: should be zero
+                    assert_abs_diff_eq!(s_pc_mains[[r, c]], 0.0, epsilon = 1e-12);
                 }
             }
-            let block_norm = s_matrix
-                .slice(s![
-                    interaction_block.col_range.clone(),
-                    interaction_block.col_range.clone()
-                ])
-                .mapv(|x| x.abs())
-                .sum();
-            assert!(block_norm > 1e-9, "Penalty block should be non-zero.");
         }
     }
 
@@ -1452,7 +1432,7 @@ mod tests {
     fn test_construction_with_no_pcs() {
         let (data, config) = create_test_data_for_construction(100, 0); // 0 PCs
 
-        let (_, _, layout, _, _) = build_design_and_penalty_matrices(&data, &config).unwrap();
+        let (_, _, layout, _, _, _) = build_design_and_penalty_matrices(&data, &config).unwrap();
 
         let pgs_main_coeffs =
             config.pgs_basis_config.num_knots + config.pgs_basis_config.degree - 1;
diff --git a/calibrate/estimate.rs b/calibrate/estimate.rs
index 120805c..9961aea 100644
--- a/calibrate/estimate.rs
+++ b/calibrate/estimate.rs
@@ -115,7 +115,7 @@ pub fn train_model(
     );
 
     eprintln!("\n[STAGE 1/3] Constructing model structure...");
-    let (x_matrix, s_list, layout, constraints, knot_vectors) =
+    let (x_matrix, s_list, layout, constraints, knot_vectors, range_transforms) =
         build_design_and_penalty_matrices(data, config)?;
     log_layout_info(&layout);
     eprintln!(
@@ -351,6 +351,7 @@ pub fn train_model(
     let mut config_with_constraints = config.clone();
     config_with_constraints.constraints = constraints;
     config_with_constraints.knot_vectors = knot_vectors;
+    config_with_constraints.range_transforms = range_transforms;
 
     Ok(TrainedModel {
         config: config_with_constraints,
@@ -1709,6 +1710,7 @@ pub mod internal {
                 pgs_range: (-3.0, 3.0),
                 constraints: HashMap::new(),
                 knot_vectors: HashMap::new(),
+            range_transforms: HashMap::new(),
             };
 
             (data, config)
@@ -1735,6 +1737,7 @@ pub mod internal {
                 pc_names: vec!["PC1".to_string()],
                 constraints: HashMap::new(),
                 knot_vectors: HashMap::new(),
+            range_transforms: HashMap::new(),
             }
         }
 
@@ -2152,10 +2155,11 @@ pub mod internal {
                 pc_names: vec!["PC1".to_string(), "PC2".to_string()],
                 constraints: std::collections::HashMap::new(),
                 knot_vectors: std::collections::HashMap::new(),
+                range_transforms: std::collections::HashMap::new(),
             };
 
             // Build the layout to programmatically find penalty indices
-            let (_, _, layout, _, _) = build_design_and_penalty_matrices(&data, &config)
+            let (_, _, layout, _, _, _) = build_design_and_penalty_matrices(&data, &config)
                 .expect("Failed to build layout for test");
 
             // Train the model
@@ -2591,10 +2595,11 @@ pub mod internal {
                 pgs_range: (-2.5, 2.5),
                 constraints: std::collections::HashMap::new(),
                 knot_vectors: std::collections::HashMap::new(),
+                range_transforms: std::collections::HashMap::new(),
             };
 
             // --- 3. Build Model Structure ---
-            let (x_matrix, mut s_list, layout, _, _) =
+            let (x_matrix, mut s_list, layout, _, _, _) =
                 build_design_and_penalty_matrices(&data, &config).unwrap();
 
             assert!(
@@ -2901,10 +2906,11 @@ pub mod internal {
                 pc_names: vec!["PC1".to_string()],
                 constraints: HashMap::new(),
                 knot_vectors: HashMap::new(),
+            range_transforms: HashMap::new(),
             };
 
             // Test with extreme lambda values that might cause issues
-            let (x_matrix, s_list, layout, _, _) =
+            let (x_matrix, s_list, layout, _, _, _) =
                 build_design_and_penalty_matrices(&data, &config).unwrap();
 
             // Try with very large lambda values (exp(10) ~ 22000)
@@ -3025,10 +3031,11 @@ pub mod internal {
                 pc_names: vec!["PC1".to_string()],
                 constraints: HashMap::new(),
                 knot_vectors: HashMap::new(),
+            range_transforms: HashMap::new(),
             };
 
             // Test that we can at least compute cost without getting infinity
-            let (x_matrix, s_list, layout, _, _) =
+            let (x_matrix, s_list, layout, _, _, _) =
                 build_design_and_penalty_matrices(&data, &config).unwrap();
 
             let reml_state = internal::RemlState::new(
@@ -3249,6 +3256,7 @@ pub mod internal {
                 pc_names: vec!["PC1".to_string()],
                 constraints: HashMap::new(),
                 knot_vectors: HashMap::new(),
+            range_transforms: HashMap::new(),
             };
 
             println!(
@@ -3347,9 +3355,10 @@ pub mod internal {
                 pc_names: vec!["PC1".to_string()],
                 constraints: HashMap::new(),
                 knot_vectors: HashMap::new(),
+            range_transforms: HashMap::new(),
             };
 
-            let (x, s_list, layout, _, _) =
+            let (x, s_list, layout, _, _, _) =
                 build_design_and_penalty_matrices(&data, &config).unwrap();
             // Explicitly drop unused variables
             // Unused variables removed
@@ -3529,10 +3538,11 @@ pub mod internal {
                 pc_names: vec!["PC1".to_string()],
                 constraints: Default::default(),
                 knot_vectors: Default::default(),
+                range_transforms: Default::default(),
             };
 
             // Build design and penalty matrices
-            let (x_matrix, s_list, layout, constraints, _) =
+            let (x_matrix, s_list, layout, constraints, _, _) =
                 internal::build_design_and_penalty_matrices(&training_data, &config)
                     .expect("Failed to build design matrix");
 
@@ -3660,7 +3670,7 @@ pub mod internal {
                 simple_config.pgs_basis_config.num_knots = 4; // Use a reasonable number of knots
 
                 // 3. Build GUARANTEED CONSISTENT structures for this simple model.
-                let (x_simple, s_list_simple, layout_simple, _, _) =
+                let (x_simple, s_list_simple, layout_simple, _, _, _) =
                     build_design_and_penalty_matrices(&data, &simple_config).unwrap_or_else(|e| {
                         panic!("Matrix build failed for {:?}: {:?}", link_function, e)
                     });
@@ -3782,7 +3792,7 @@ pub mod internal {
                 simple_config.pgs_basis_config.num_knots = 3;
 
                 // 2. Generate consistent structures using the canonical function
-                let (x_simple, s_list_simple, layout_simple, _, _) =
+                let (x_simple, s_list_simple, layout_simple, _, _, _) =
                     build_design_and_penalty_matrices(&data, &simple_config).unwrap_or_else(|e| {
                         panic!("Matrix build failed for {:?}: {:?}", link_function, e)
                     });
@@ -3960,7 +3970,7 @@ pub mod internal {
             };
 
             // 2. Generate consistent structures using the canonical function
-            let (x_simple, s_list_simple, layout_simple, _, _) =
+            let (x_simple, s_list_simple, layout_simple, _, _, _) =
                 build_design_and_penalty_matrices(&data, &simple_config)
                     .unwrap_or_else(|e| panic!("Matrix build failed: {:?}", e));
 
@@ -4109,7 +4119,7 @@ pub mod internal {
             simple_config.pgs_basis_config.num_knots = 3;
 
             // 2. Generate consistent structures using the canonical function
-            let (x_simple, s_list_simple, layout_simple, _, _) =
+            let (x_simple, s_list_simple, layout_simple, _, _, _) =
                 build_design_and_penalty_matrices(&data, &simple_config)
                     .unwrap_or_else(|e| panic!("Matrix build failed: {:?}", e));
 
@@ -4213,7 +4223,7 @@ pub mod internal {
             simple_config.pgs_basis_config.num_knots = 3;
 
             // 2. Generate consistent structures using the canonical function
-            let (x_simple, s_list_simple, layout_simple, _, _) =
+            let (x_simple, s_list_simple, layout_simple, _, _, _) =
                 build_design_and_penalty_matrices(&data, &simple_config)
                     .unwrap_or_else(|e| panic!("Matrix build failed: {:?}", e));
 
@@ -4312,6 +4322,7 @@ fn test_train_model_fails_gracefully_on_perfect_separation() {
         pc_ranges: vec![],
         constraints: HashMap::new(),
         knot_vectors: HashMap::new(),
+            range_transforms: HashMap::new(),
     };
 
     // 3. Train the model and expect an error
@@ -4385,11 +4396,12 @@ fn test_indefinite_hessian_detection_and_retreat() {
         pc_names: vec!["PC1".to_string()],
         constraints: std::collections::HashMap::new(),
         knot_vectors: std::collections::HashMap::new(),
+        range_transforms: std::collections::HashMap::new(),
     };
 
     // Try to build the matrices - if this fails, the test is still valid
     let matrices_result = build_design_and_penalty_matrices(&data, &config);
-    if let Ok((x_matrix, s_list, layout, _, _)) = matrices_result {
+    if let Ok((x_matrix, s_list, layout, _, _, _)) = matrices_result {
         let reml_state_result = RemlState::new(
             data.y.view(),
             x_matrix.view(),
@@ -4584,10 +4596,11 @@ mod optimizer_progress_tests {
             pc_names: vec!["PC1".to_string()],
             constraints: std::collections::HashMap::new(),
             knot_vectors: std::collections::HashMap::new(),
+            range_transforms: std::collections::HashMap::new(),
         };
 
         // 3) Build matrices and REML state to evaluate cost at specific rho
-        let (x_matrix, s_list, layout, _, _) = build_design_and_penalty_matrices(&data, &config)?;
+        let (x_matrix, s_list, layout, _, _, _) = build_design_and_penalty_matrices(&data, &config)?;
         let reml_state = internal::RemlState::new(
             data.y.view(),
             x_matrix.view(),
diff --git a/calibrate/model.rs b/calibrate/model.rs
index c2d5535..f4f3186 100644
--- a/calibrate/model.rs
+++ b/calibrate/model.rs
@@ -62,6 +62,10 @@ pub struct ModelConfig {
     pub constraints: HashMap<String, Constraint>,
     /// Knot vectors used during training, required for exact reproduction during prediction
     pub knot_vectors: HashMap<String, Array1<f64>>,
+    /// Range transformation matrices for functional ANOVA decomposition
+    /// Maps variable names (e.g., "pgs", "PC1") to their range-space transformation matrices
+    #[serde(default)] // For backward compatibility with models that don't have this field
+    pub range_transforms: HashMap<String, Array2<f64>>,
 }
 
 /// A structured representation of the fitted model coefficients, designed for
@@ -275,8 +279,8 @@ mod internal {
         // This closure was not used - removed
         // Previously defined a helper to fetch Z transform from model
 
-        // 2. Generate bases for PCs using saved knot vectors if available
-        let mut pc_constrained_bases = Vec::new();
+        // 2. Generate range-only bases for PCs (functional ANOVA approach)
+        let mut pc_range_bases = Vec::new();
         let mut pc_unconstrained_bases_main = Vec::new();
         for i in 0..config.pc_names.len() {
             let pc_col = pcs_new.column(i);
@@ -297,20 +301,26 @@ mod internal {
             let pc_main_basis_unc = pc_basis_unc.slice(s![.., 1..]);
             pc_unconstrained_bases_main.push(pc_main_basis_unc.to_owned());
 
-            // Apply the SAVED PC constraint
-            let pc_name = &config.pc_names[i];
-            let pc_z = &config
-                .constraints
-                .get(pc_name)
-                .ok_or_else(|| ModelError::ConstraintMissing(pc_name.clone()))?
-                .z_transform;
+            // Apply the SAVED range transformation for functional ANOVA
+            if let Some(range_transform) = config.range_transforms.get(pc_name) {
+                // Use range-only transformation for PC main effects (fully penalized)
+                let pc_range_basis = pc_main_basis_unc.dot(range_transform);
+                pc_range_bases.push(pc_range_basis);
+            } else {
+                // Fallback to old constraint approach for backward compatibility
+                let pc_z = &config
+                    .constraints
+                    .get(pc_name)
+                    .ok_or_else(|| ModelError::ConstraintMissing(pc_name.clone()))?
+                    .z_transform;
+
+                // Check that dimensions match before matrix multiplication
+                if pc_main_basis_unc.ncols() != pc_z.nrows() {
+                    return Err(ModelError::InternalStackingError);
+                }
 
-            // Check that dimensions match before matrix multiplication
-            if pc_main_basis_unc.ncols() != pc_z.nrows() {
-                return Err(ModelError::InternalStackingError);
+                pc_range_bases.push(pc_main_basis_unc.dot(pc_z)); // Fallback constrained
             }
-
-            pc_constrained_bases.push(pc_main_basis_unc.dot(pc_z)); // Now constrained
         }
 
         // 3. Assemble the design matrix following the canonical order
@@ -320,8 +330,8 @@ mod internal {
         // 1. Intercept
         owned_cols.push(Array1::ones(n_samples));
 
-        // 2. Main PC effects
-        for pc_basis in &pc_constrained_bases {
+        // 2. Main PC effects (using range-only bases for functional ANOVA)
+        for pc_basis in &pc_range_bases {
             for col in pc_basis.axis_iter(Axis(1)) {
                 owned_cols.push(col.to_owned());
             }
@@ -332,27 +342,56 @@ mod internal {
             owned_cols.push(col.to_owned());
         }
 
-        // 4. Tensor product interaction effects (conditionally)
-        // Only create interaction terms if we have PCs and the UNCONSTRAINED PGS basis has columns
-        // Gate on the same basis dimension used to construct the layout (unconstrained)
-        let num_pgs_interaction_bases = pgs_main_basis_unc.ncols();
-        if num_pgs_interaction_bases > 0 && !config.pc_names.is_empty() {
-            // Create unified interaction surfaces using row-wise tensor products
+        // 4. Tensor product interaction effects - Range × Range only (functional ANOVA)
+        if let Some(pgs_range_transform) = config.range_transforms.get("pgs") {
+            let pgs_range_basis = pgs_main_basis_unc.dot(pgs_range_transform);
+            
             for pc_idx in 0..config.pc_names.len() {
-                let pc_unconstrained_basis = &pc_unconstrained_bases_main[pc_idx];
                 let pc_name = &config.pc_names[pc_idx];
                 let tensor_key = format!("f(PGS,{})", pc_name);
 
                 // Only build the interaction if the trained model contains coefficients for it
                 if coeffs.interaction_effects.contains_key(&tensor_key) {
-                    let tensor_interaction = row_wise_tensor_product(
-                        &pgs_main_basis_unc.to_owned(),
-                        pc_unconstrained_basis,
-                    );
-
-                    // Add all columns from this tensor product to the design matrix
-                    for col in tensor_interaction.axis_iter(Axis(1)) {
-                        owned_cols.push(col.to_owned());
+                    if let Some(pc_range_transform) = config.range_transforms.get(pc_name) {
+                        // Use Range × Range tensor product (fully penalized by construction)
+                        let pc_range_basis = pc_unconstrained_bases_main[pc_idx].dot(pc_range_transform);
+                        let tensor_interaction = row_wise_tensor_product(&pgs_range_basis, &pc_range_basis);
+
+                        // Add all columns from this tensor product to the design matrix
+                        for col in tensor_interaction.axis_iter(Axis(1)) {
+                            owned_cols.push(col.to_owned());
+                        }
+                    } else {
+                        // Fallback to old approach for backward compatibility
+                        let tensor_interaction = row_wise_tensor_product(
+                            &pgs_main_basis_unc.to_owned(),
+                            &pc_unconstrained_bases_main[pc_idx],
+                        );
+
+                        for col in tensor_interaction.axis_iter(Axis(1)) {
+                            owned_cols.push(col.to_owned());
+                        }
+                    }
+                }
+            }
+        } else {
+            // Fallback to old approach for backward compatibility
+            let num_pgs_interaction_bases = pgs_main_basis_unc.ncols();
+            if num_pgs_interaction_bases > 0 && !config.pc_names.is_empty() {
+                for pc_idx in 0..config.pc_names.len() {
+                    let pc_unconstrained_basis = &pc_unconstrained_bases_main[pc_idx];
+                    let pc_name = &config.pc_names[pc_idx];
+                    let tensor_key = format!("f(PGS,{})", pc_name);
+
+                    if coeffs.interaction_effects.contains_key(&tensor_key) {
+                        let tensor_interaction = row_wise_tensor_product(
+                            &pgs_main_basis_unc.to_owned(),
+                            pc_unconstrained_basis,
+                        );
+
+                        for col in tensor_interaction.axis_iter(Axis(1)) {
+                            owned_cols.push(col.to_owned());
+                        }
                     }
                 }
             }
@@ -462,6 +501,7 @@ mod tests {
                     knots.insert("pgs".to_string(), knot_vector.clone());
                     knots
                 },
+                range_transforms: HashMap::new(),
             },
             coefficients: MappedCoefficients {
                 intercept: 0.5, // Added an intercept for a more complete test
@@ -530,6 +570,7 @@ mod tests {
                 pc_names: vec!["PC1".to_string()],
                 constraints: HashMap::new(),
                 knot_vectors: HashMap::new(),
+            range_transforms: HashMap::new(),
             },
             coefficients: MappedCoefficients {
                 intercept: 0.0,
@@ -617,6 +658,7 @@ mod tests {
             pc_names: vec!["PC1".to_string(), "PC2".to_string()], // Order matters for flattening
             constraints: HashMap::new(),
             knot_vectors: HashMap::new(),
+            range_transforms: HashMap::new(),
         };
 
         // Use the internal flatten_coefficients function
@@ -693,6 +735,7 @@ mod tests {
             pc_names: vec!["PC1".to_string()],
             constraints: HashMap::new(), // Will be populated by build_design_and_penalty_matrices
             knot_vectors: HashMap::new(), // Will be populated by build_design_and_penalty_matrices
+            range_transforms: HashMap::new(), // Will be populated by build_design_and_penalty_matrices
         };
 
         // Create a dummy dataset for generating the correct model structure
@@ -709,8 +752,8 @@ mod tests {
             weights: Array1::ones(n_samples),
         };
 
-        // Generate the correct constraints and structure using the actual model-building code
-        let (_, _, layout, constraints, knot_vectors) =
+        // Generate the correct constraints, structure, and range transforms using the actual model-building code
+        let (_, _, layout, constraints, knot_vectors, range_transforms) =
             crate::calibrate::construction::build_design_and_penalty_matrices(
                 &dummy_data,
                 &model_config,
@@ -733,6 +776,7 @@ mod tests {
                 pc_names: vec!["PC1".to_string()],
                 constraints: constraints.clone(), // Clone to avoid ownership issues
                 knot_vectors, // Use the knot vectors generated by the model-building code
+                range_transforms: range_transforms.clone(), // Use full Option 3 pipeline
             },
             coefficients: MappedCoefficients {
                 intercept: 0.5,
@@ -745,12 +789,8 @@ mod tests {
                         (1..=pgs_dim).map(|i| i as f64).collect()
                     },
                     pcs: {
-                        // Extract PC1 dimension from constraints
-                        let pc1_dim = if let Some(pc1_constraint) = constraints.get("PC1") {
-                            pc1_constraint.z_transform.ncols()
-                        } else {
-                            9 // Default fallback
-                        };
+                        // Use range dimension for PC1 (Option 3)
+                        let pc1_dim = range_transforms.get("PC1").unwrap().ncols();
 
                         let mut pc_map = HashMap::new();
                         pc_map.insert("PC1".to_string(), (1..=pc1_dim).map(|i| i as f64).collect());
@@ -758,18 +798,14 @@ mod tests {
                     },
                 },
                 interaction_effects: {
-                    // The interaction term's size depends on the UNCONSTRAINED dimensions of the marginal bases' main effect parts.
-                    // Total basis functions = num_knots + degree + 1. Main effect part is one less.
-                    let num_pgs_basis_funcs_main =
-                        pgs_basis_config.num_knots + pgs_basis_config.degree; // 9
-                    let num_pc1_basis_funcs_main =
-                        pc1_basis_config.num_knots + pc1_basis_config.degree; // 9
+                    // Option 3: Interaction term uses Range × Range dimensions
+                    let r_pgs = range_transforms.get("pgs").unwrap().ncols();
+                    let r_pc1 = range_transforms.get("PC1").unwrap().ncols();
 
                     let mut interactions = HashMap::new();
 
-                    // Calculate the total number of interaction coefficients for the unified term
-                    let total_interaction_coeffs =
-                        num_pgs_basis_funcs_main * num_pc1_basis_funcs_main; // CORRECT: 9 * 9 = 81
+                    // Calculate the total number of interaction coefficients for Range × Range
+                    let total_interaction_coeffs = r_pgs * r_pc1;
 
                     // Create a single flattened vector of coefficients
                     let interaction_coeffs: Vec<f64> = (1..=total_interaction_coeffs)
@@ -781,7 +817,7 @@ mod tests {
                     interactions
                 },
             },
-            lambdas: vec![0.1, 0.2],
+            lambdas: vec![0.1, 0.2], // Option 3: 2 grouped penalties (PC mains, interactions)
         };
 
         // Create a temporary file for testing
diff --git a/calibrate/pirls.rs b/calibrate/pirls.rs
index dd5900f..0fdcca6 100644
--- a/calibrate/pirls.rs
+++ b/calibrate/pirls.rs
@@ -1850,6 +1850,7 @@ mod tests {
             pc_names: vec![],
             constraints: HashMap::new(),
             knot_vectors: HashMap::new(),
+            range_transforms: HashMap::new(),
         };
 
         // --- 3. Run the Fit ---
@@ -2053,7 +2054,7 @@ mod tests {
         data: &TrainingData,
         config: &ModelConfig,
     ) -> Result<(Array2<f64>, Vec<Array2<f64>>, ModelLayout), Box<dyn std::error::Error>> {
-        let (x_matrix, s_list, layout, _, _) = build_design_and_penalty_matrices(data, config)?;
+        let (x_matrix, s_list, layout, _, _, _) = build_design_and_penalty_matrices(data, config)?;
         let rs_original = compute_penalty_square_roots(&s_list)?;
         Ok((x_matrix, rs_original, layout))
     }
@@ -2141,6 +2142,7 @@ mod tests {
             pc_names: vec![],
             constraints: HashMap::new(),
             knot_vectors: HashMap::new(),
+            range_transforms: HashMap::new(),
         };
 
         // Test with lambda values that match the working test pattern
@@ -2261,6 +2263,7 @@ mod tests {
             pc_names: vec![],
             constraints: HashMap::new(),
             knot_vectors: HashMap::new(),
+            range_transforms: HashMap::new(),
         };
 
         // === PHASE 4: Prepare inputs for the target function ===
@@ -2385,6 +2388,7 @@ mod tests {
             pc_names: vec![],
             constraints: HashMap::new(),
             knot_vectors: HashMap::new(),
+            range_transforms: HashMap::new(),
         };
 
         // === Set up inputs using helper ===
diff --git a/calibrate/tests/run_mgcv.R b/calibrate/tests/run_mgcv.R
index 488a28f..58f892a 100644
--- a/calibrate/tests/run_mgcv.R
+++ b/calibrate/tests/run_mgcv.R
@@ -3,42 +3,41 @@ if (!require(mgcv)) {
   library(mgcv)
 }
 
-# Define file paths for clarity
-input_csv_file <- 'synthetic_classification_data.csv'
-output_rds_file <- 'gam_model_fit.rds'
-
+# --- 1. I/O ---
+input_csv_file <- "synthetic_classification_data.csv"
+output_rds_file <- "gam_model_fit.rds"
 
 # --- 2. Load Data ---
-cat("Loading data from '", input_csv_file, "'...\n\n", sep="")
+cat("Loading data from '", input_csv_file, "'...\n\n", sep = "")
 data <- read.csv(input_csv_file)
 
-
 # --- 3. Define and Fit the GAM ---
-
-gam_formula <- outcome ~ s(variable_one, bs = "ps", k = 11, fx = TRUE, m = c(4, 2)) +
-                                     s(variable_two, bs = "ps", k = 12, m = c(4, 2)) +
-                                     ti(variable_one, variable_two, bs = c("ps", "ps"), k = c(12, 12), m = c(4, 2))
-
-cat("Fitting the GAM... This may take a moment.\n")
-# Fit the model using REML for smoothness selection
-# The family is 'binomial' because our outcome is 0 or 1.
+# Goal:
+# - PGS main effect (variable_one): unpenalized (fixed df) -> fx=TRUE
+#   Use k=11 so the effective columns are ~10 after mgcv's identifiability handling.
+# - PC1 main effect (variable_two): penalized, but match Rust's 10 cols -> set k=11 (not 12).
+# - Interaction: penalized, anisotropic (2 smoothing params) via ti().
+#   With k=c(12,12) and penalty order 2, ti() builds the "interaction-only" 100-column block.
+
+gam_formula <- outcome ~
+  s(variable_one, bs = "ps", k = 11, m = c(4, 2), fx = TRUE) +              # unpenalized PGS main
+  s(variable_two, bs = "ps", k = 11, m = c(4, 2)) +                         # penalized PC1 main (≈10 cols)
+  ti(variable_one, variable_two, bs = c("ps", "ps"), k = c(12, 12), m = c(4, 2))  # penalized interaction (2 λ)
+
+cat("Fitting the GAM (REML)...\n")
 gam_fit <- gam(
   gam_formula,
-  family = binomial(),
+  family = binomial(link = "logit"),
   data = data,
-  method = "REML" # Restricted Maximum Likelihood is a good default
+  method = "REML"
 )
 
-
 # --- 4. Inspect the Model Fit ---
-
 cat("\n--- GAM Model Summary ---\n")
 print(summary(gam_fit))
 
-
 # --- 5. Save the Model Object ---
-
-cat("\nSaving the fitted GAM object to '", output_rds_file, "'...\n", sep="")
+cat("\nSaving the fitted GAM object to '", output_rds_file, "'...\n", sep = "")
 saveRDS(gam_fit, file = output_rds_file)
-
 cat("Script finished successfully.\n\n")
+
